"""
3D Landmarks Visualizer

This module provides functionality to visualize sign language landmarks in 3D space
from video_landmarks.json files generated by the processing pipeline.

Usage:
    from visualizer_3d import visualize_landmarks_3d
    visualize_landmarks_3d("path/to/video_landmarks.json")
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import argparse
from collections import defaultdict, deque
import cv2

#TODO coordinates for pose and hands are disconnected

class LandmarksVisualizer3D:
    """3D visualizer for sign language landmarks"""

    def __init__(self, json_path: str, frame_rate: float = 10.0, track_hands: bool = False):
        """
        Initialize the 3D visualizer

        Args:
            json_path: Path to video_landmarks.json file
            frame_rate: Animation frame rate (frames per second)
            track_hands: Whether to enable hand path tracking with advanced consistency correction
        """
        self.json_path = Path(json_path)
        self.frame_rate = frame_rate
        self.track_hands = track_hands
        self.data = None
        self.frames_data = None
        self.metadata = None
        self.current_frame = 0

        # Initialize hand path tracker if enabled
        self.hand_tracker = None

        # Color scheme for different landmark types
        self.colors = {
            'hands': {'left_hand': 'red', 'right_hand': 'blue'},
            'face': 'yellow',
            'pose': 'green'
        }

        # Load data
        self._load_data()

        # Initialize hand tracking after data is loaded
        if track_hands:
            self.hand_tracker = HandPathTracker()
            self.hand_tracker.precompute_paths_from_json(self.frames_data)

            # Print path statistics
            stats = self.hand_tracker.get_path_statistics()
            correction_info = stats['consistency_corrections']
            print("\nHand Path Statistics (with advanced consistency correction):")
            print(f"  Advanced corrections applied: {correction_info['corrections_made']}")
            print(f"  Correction rate: {correction_info['correction_rate']:.1f}% of frames")
            print(f"  Left hand: {stats['left_hand']['total_points']} points, "
                  f"distance: {stats['left_hand']['total_distance']:.3f}, "
                  f"avg speed: {stats['left_hand']['avg_speed']:.3f}")
            print(f"  Right hand: {stats['right_hand']['total_points']} points, "
                  f"distance: {stats['right_hand']['total_distance']:.3f}, "
                  f"avg speed: {stats['right_hand']['avg_speed']:.3f}")

    def _load_data(self):
        """Load landmarks data from JSON file"""
        if not self.json_path.exists():
            raise FileNotFoundError(f"JSON file not found: {self.json_path}")

        try:
            with open(self.json_path, 'r') as f:
                self.data = json.load(f)

            self.metadata = self.data.get('metadata', {})
            self.frames_data = self.data.get('frames', {})

            if not self.frames_data:
                raise ValueError("No frames data found in JSON file")

            print(f"Loaded data for {len(self.frames_data)} frames")
            print(f"Source: {self.metadata.get('input_source', 'Unknown')}")
            print(f"FPS: {self.metadata.get('fps', 'Unknown')}")

        except Exception as e:
            raise Exception(f"Error loading JSON file: {e}")


    def _setup_3d_plot(self):
        """Set up the 3D matplotlib plot"""
        self.fig = plt.figure(figsize=(12, 9))
        self.ax = self.fig.add_subplot(111, projection='3d')

        # Set labels and title
        self.ax.set_xlabel('X')
        self.ax.set_ylabel('Y')
        self.ax.set_zlabel('Z')
        self.ax.set_title(f'3D Landmarks Visualization\nSource: {self.metadata.get("input_source", "Unknown")}')

        # Set axis limits (normalized coordinates are 0-1)
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 1)
        self.ax.set_zlim(-0.5, 0.5)  # Z coordinates are typically smaller

        # Invert Y axis to match image coordinates
        self.ax.invert_yaxis()

        return self.ax



    def _draw_hand_connections(self, points: np.ndarray, color: str):
        """Draw connections between hand landmarks"""
        if len(points) != 21:  # MediaPipe hand has 21 landmarks
            return

        # Define hand connections (simplified version)
        connections = [
            # Thumb
            (0, 1), (1, 2), (2, 3), (3, 4),
            # Index finger
            (0, 5), (5, 6), (6, 7), (7, 8),
            # Middle finger
            (0, 9), (9, 10), (10, 11), (11, 12),
            # Ring finger
            (0, 13), (13, 14), (14, 15), (15, 16),
            # Pinky
            (0, 17), (17, 18), (18, 19), (19, 20),
            # Palm connections
            (5, 9), (9, 13), (13, 17)
        ]

        for start_idx, end_idx in connections:
            start_point = points[start_idx]
            end_point = points[end_idx]
            self.ax.plot([start_point[0], end_point[0]],
                         [start_point[1], end_point[1]],
                         [start_point[2], end_point[2]],
                         color=color, alpha=0.5, linewidth=1)

    def _draw_pose_connections(self, points: np.ndarray, pose_names: list = None):
        """Draw connections between pose landmarks using MediaPipe's pose structure"""
        if len(points) == 0:
            return

        # Define MediaPipe pose connections (index pairs)
        # These are the standard connections used by MediaPipe
        mediapipe_pose_connections = [
            # Face connections
            ('NOSE', 'LEFT_EYE_INNER'), ('LEFT_EYE_INNER', 'LEFT_EYE'),
            ('LEFT_EYE', 'LEFT_EYE_OUTER'), ('LEFT_EYE_OUTER', 'LEFT_EAR'),
            ('NOSE', 'RIGHT_EYE_INNER'), ('RIGHT_EYE_INNER', 'RIGHT_EYE'),
            ('RIGHT_EYE', 'RIGHT_EYE_OUTER'), ('RIGHT_EYE_OUTER', 'RIGHT_EAR'),
            ('MOUTH_LEFT', 'MOUTH_RIGHT'),

            # Torso connections
            ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
            ('LEFT_SHOULDER', 'LEFT_HIP'), ('RIGHT_SHOULDER', 'RIGHT_HIP'),
            ('LEFT_HIP', 'RIGHT_HIP'),

            # Left arm connections
            ('LEFT_SHOULDER', 'LEFT_ELBOW'), ('LEFT_ELBOW', 'LEFT_WRIST'),
            ('LEFT_WRIST', 'LEFT_PINKY'), ('LEFT_WRIST', 'LEFT_INDEX'),
            ('LEFT_WRIST', 'LEFT_THUMB'), ('LEFT_PINKY', 'LEFT_INDEX'),

            # Right arm connections
            ('RIGHT_SHOULDER', 'RIGHT_ELBOW'), ('RIGHT_ELBOW', 'RIGHT_WRIST'),
            ('RIGHT_WRIST', 'RIGHT_PINKY'), ('RIGHT_WRIST', 'RIGHT_INDEX'),
            ('RIGHT_WRIST', 'RIGHT_THUMB'), ('RIGHT_PINKY', 'RIGHT_INDEX'),

            # Left leg connections
            ('LEFT_HIP', 'LEFT_KNEE'), ('LEFT_KNEE', 'LEFT_ANKLE'),
            ('LEFT_ANKLE', 'LEFT_HEEL'), ('LEFT_ANKLE', 'LEFT_FOOT_INDEX'),
            ('LEFT_HEEL', 'LEFT_FOOT_INDEX'),

            # Right leg connections
            ('RIGHT_HIP', 'RIGHT_KNEE'), ('RIGHT_KNEE', 'RIGHT_ANKLE'),
            ('RIGHT_ANKLE', 'RIGHT_HEEL'), ('RIGHT_ANKLE', 'RIGHT_FOOT_INDEX'),
            ('RIGHT_HEEL', 'RIGHT_FOOT_INDEX'),
        ]

        # If we don't have pose names, fall back to simplified connections
        if pose_names is None or len(pose_names) != len(points):
            # Simplified fallback: connect major body parts if we have enough points
            if len(points) >= 4:
                # Try to connect what we assume are major landmarks
                color = self.colors['pose']
                # Connect first few points (assuming they're major landmarks)
                for i in range(min(4, len(points) - 1)):
                    self.ax.plot([points[i][0], points[i + 1][0]],
                                 [points[i][1], points[i + 1][1]],
                                 [points[i][2], points[i + 1][2]],
                                 color=color, alpha=0.6, linewidth=2)
            return

        # Create a mapping from landmark names to indices
        name_to_index = {name: i for i, name in enumerate(pose_names)}

        # Draw connections based on MediaPipe's structure
        color = self.colors['pose']
        connections_drawn = 0

        for start_name, end_name in mediapipe_pose_connections:
            if start_name in name_to_index and end_name in name_to_index:
                start_idx = name_to_index[start_name]
                end_idx = name_to_index[end_name]

                start_point = points[start_idx]
                end_point = points[end_idx]

                # Draw the connection
                self.ax.plot([start_point[0], end_point[0]],
                             [start_point[1], end_point[1]],
                             [start_point[2], end_point[2]],
                             color=color, alpha=0.7, linewidth=2)
                connections_drawn += 1

        # If no connections were drawn, fall back to simplified approach
        if connections_drawn == 0 and len(points) >= 2:
            print(f"Warning: No pose connections found, using simplified visualization")
            # Connect shoulders if we can identify them
            shoulder_connections = [
                ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
                ('LEFT_SHOULDER', 'LEFT_ELBOW'),
                ('RIGHT_SHOULDER', 'RIGHT_ELBOW')
            ]

            for start_name, end_name in shoulder_connections:
                if start_name in name_to_index and end_name in name_to_index:
                    start_idx = name_to_index[start_name]
                    end_idx = name_to_index[end_name]

                    start_point = points[start_idx]
                    end_point = points[end_idx]

                    self.ax.plot([start_point[0], end_point[0]],
                                 [start_point[1], end_point[1]],
                                 [start_point[2], end_point[2]],
                                 color=color, alpha=0.7, linewidth=2)

    # def _extract_landmarks_for_frame(self, frame_key: str) -> Dict:
    #     """Extract 3D coordinates for a specific frame"""
    #     frame_data = self.frames_data.get(frame_key, {})
    #     landmarks_3d = {
    #         'hands': {'left_hand': [], 'right_hand': []},
    #         'face': [],
    #         'pose': []
    #     }
    #
    #     # Extract hand landmarks
    #     hands_data = frame_data.get('hands', {})
    #     for hand_type in ['left_hand', 'right_hand']:
    #         hand_info = hands_data.get(hand_type, {})
    #         if isinstance(hand_info, dict) and 'landmarks' in hand_info:
    #             # New format with confidence
    #             hand_landmarks = hand_info['landmarks']
    #         elif isinstance(hand_info, list):
    #             # Old format - direct list
    #             hand_landmarks = hand_info
    #         else:
    #             hand_landmarks = []
    #
    #         if hand_landmarks:
    #             points = np.array([[lm['x'], lm['y'], lm['z']] for lm in hand_landmarks])
    #             landmarks_3d['hands'][hand_type] = points
    #
    #     # Extract face landmarks (using all landmarks)
    #     face_data = frame_data.get('face', {})
    #     if 'all_landmarks' in face_data and face_data['all_landmarks']:
    #         # Use a subset of face landmarks for better visualization
    #         face_landmarks = face_data['all_landmarks']
    #         # Take every 10th landmark to reduce clutter
    #         sampled_landmarks = face_landmarks[::2]
    #         points = np.array([[lm['x'], lm['y'], lm['z']] for lm in sampled_landmarks])
    #         landmarks_3d['face'] = points
    #
    #     # Extract pose landmarks with proper ordering
    #     pose_data = frame_data.get('pose', {})
    #     if pose_data:
    #         # Define the complete MediaPipe pose landmark order
    #         mediapipe_pose_landmarks = [
    #             'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER',
    #             'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT',
    #             'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',
    #             'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX',
    #             'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP',
    #             'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL',
    #             'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'
    #         ]
    #
    #         # Extract landmarks in the correct order with visibility check
    #         pose_points = []
    #         pose_landmark_names = []
    #
    #         for landmark_name in mediapipe_pose_landmarks:
    #             if landmark_name in pose_data:
    #                 lm = pose_data[landmark_name]
    #                 # Check visibility if available
    #                 visibility = lm.get('visibility', 1.0)
    #                 if visibility > 0.5:  # Only include visible landmarks
    #                     pose_points.append([lm['x'], lm['y'], lm['z']])
    #                     pose_landmark_names.append(landmark_name)
    #
    #         if pose_points:
    #             landmarks_3d['pose'] = np.array(pose_points)
    #             landmarks_3d['pose_names'] = pose_landmark_names
    #
    #     return landmarks_3d

    def _extract_landmarks_for_frame_corrected(self, frame_key: str) -> Dict:
        """Extract 3D coordinates for a specific frame using corrected hand data with hand-wrist calibration"""
        if self.hand_tracker and hasattr(self.hand_tracker, 'corrected_frames_data'):
            # Use corrected frame data if available
            frame_data = self.hand_tracker.get_corrected_frame_data(frame_key)
        else:
            # Fall back to original data
            frame_data = self.frames_data.get(frame_key, {})

        landmarks_3d = {
            'hands': {'left_hand': [], 'right_hand': []},
            'face': [],
            'pose': []
        }

        # First, extract pose landmarks to get wrist positions for calibration
        pose_wrists = {"LEFT_WRIST": None, "RIGHT_WRIST": None}
        pose_data = frame_data.get('pose', {})

        if pose_data:
            if "LEFT_WRIST" in pose_data:
                pose_wrists["LEFT_WRIST"] = np.array([
                    pose_data["LEFT_WRIST"]["x"],
                    pose_data["LEFT_WRIST"]["y"],
                    pose_data["LEFT_WRIST"]["z"]
                ])

            if "RIGHT_WRIST" in pose_data:
                pose_wrists["RIGHT_WRIST"] = np.array([
                    pose_data["RIGHT_WRIST"]["x"],
                    pose_data["RIGHT_WRIST"]["y"],
                    pose_data["RIGHT_WRIST"]["z"]
                ])

        # Extract and calibrate hand landmarks
        hands_data = frame_data.get('hands', {})
        hand_to_wrist_mapping = {
            'left_hand': 'LEFT_WRIST',
            'right_hand': 'RIGHT_WRIST'
        }

        for hand_type in ['left_hand', 'right_hand']:
            hand_info = hands_data.get(hand_type, {})
            wrist_name = hand_to_wrist_mapping[hand_type]
            pose_wrist_position = pose_wrists.get(wrist_name)

            if isinstance(hand_info, dict) and 'landmarks' in hand_info:
                # New format with confidence
                hand_landmarks = hand_info['landmarks']
            elif isinstance(hand_info, list):
                # Old format - direct list
                hand_landmarks = hand_info
            else:
                hand_landmarks = []

            if hand_landmarks:
                # Convert to numpy array for easier manipulation
                points = np.array([[lm['x'], lm['y'], lm['z']] for lm in hand_landmarks])

                # Calibrate hand position to pose wrist if both are available
                if pose_wrist_position is not None and len(points) > 0:
                    # Hand wrist is landmark 0
                    hand_wrist_position = points[0]

                    # Calculate translation offset
                    translation_offset = pose_wrist_position - hand_wrist_position

                    # Apply translation to all hand landmarks
                    points = points + translation_offset

                landmarks_3d['hands'][hand_type] = points

        # Extract face landmarks (using original method since face doesn't have calibration issues)
        face_data = frame_data.get('face', {})
        if 'all_landmarks' in face_data and face_data['all_landmarks']:
            face_landmarks = face_data['all_landmarks']
            # Take every 2nd landmark to reduce clutter
            sampled_landmarks = face_landmarks[::2]
            points = np.array([[lm['x'], lm['y'], lm['z']] for lm in sampled_landmarks])
            landmarks_3d['face'] = points

        # Extract pose landmarks with proper ordering
        if pose_data:
            # Define the complete MediaPipe pose landmark order
            mediapipe_pose_landmarks = [
                'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER',
                'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT',
                'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',
                'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX',
                'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP',
                'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL',
                'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'
            ]

            # Extract landmarks in the correct order with visibility check
            pose_points = []
            pose_landmark_names = []

            for landmark_name in mediapipe_pose_landmarks:
                if landmark_name in pose_data:
                    lm = pose_data[landmark_name]
                    # Check visibility if available
                    visibility = lm.get('visibility', 1.0)
                    if visibility > 0.5:  # Only include visible landmarks
                        pose_points.append([lm['x'], lm['y'], lm['z']])
                        pose_landmark_names.append(landmark_name)

            if pose_points:
                landmarks_3d['pose'] = np.array(pose_points)
                landmarks_3d['pose_names'] = pose_landmark_names

        return landmarks_3d

    def _extract_landmarks_for_frame(self, frame_key: str) -> Dict:
        """Extract 3D coordinates for a specific frame WITH CALIBRATION (applied to base method too)"""
        frame_data = self.frames_data.get(frame_key, {})

        landmarks_3d = {
            'hands': {'left_hand': [], 'right_hand': []},
            'face': [],
            'pose': []
        }

        # First, extract pose landmarks to get wrist positions for calibration
        pose_wrists = {"LEFT_WRIST": None, "RIGHT_WRIST": None}
        pose_data = frame_data.get('pose', {})

        if pose_data:
            if "LEFT_WRIST" in pose_data:
                pose_wrists["LEFT_WRIST"] = np.array([
                    pose_data["LEFT_WRIST"]["x"],
                    pose_data["LEFT_WRIST"]["y"],
                    pose_data["LEFT_WRIST"]["z"]
                ])

            if "RIGHT_WRIST" in pose_data:
                pose_wrists["RIGHT_WRIST"] = np.array([
                    pose_data["RIGHT_WRIST"]["x"],
                    pose_data["RIGHT_WRIST"]["y"],
                    pose_data["RIGHT_WRIST"]["z"]
                ])

        # Extract and calibrate hand landmarks
        hands_data = frame_data.get('hands', {})
        hand_to_wrist_mapping = {
            'left_hand': 'LEFT_WRIST',
            'right_hand': 'RIGHT_WRIST'
        }

        for hand_type in ['left_hand', 'right_hand']:
            hand_info = hands_data.get(hand_type, {})
            wrist_name = hand_to_wrist_mapping[hand_type]
            pose_wrist_position = pose_wrists.get(wrist_name)

            if isinstance(hand_info, dict) and 'landmarks' in hand_info:
                # New format with confidence
                hand_landmarks = hand_info['landmarks']
            elif isinstance(hand_info, list):
                # Old format - direct list
                hand_landmarks = hand_info
            else:
                hand_landmarks = []

            if hand_landmarks:
                # Convert to numpy array for easier manipulation
                points = np.array([[lm['x'], lm['y'], lm['z']] for lm in hand_landmarks])

                # Calibrate hand position to pose wrist if both are available
                if pose_wrist_position is not None and len(points) > 0:
                    # Hand wrist is landmark 0
                    hand_wrist_position = points[0]

                    # Calculate translation offset
                    translation_offset = pose_wrist_position - hand_wrist_position

                    # Apply translation to all hand landmarks
                    points = points + translation_offset

                landmarks_3d['hands'][hand_type] = points

        # Extract face landmarks (using all landmarks)
        face_data = frame_data.get('face', {})
        if 'all_landmarks' in face_data and face_data['all_landmarks']:
            # Use a subset of face landmarks for better visualization
            face_landmarks = face_data['all_landmarks']
            # Take every 10th landmark to reduce clutter
            sampled_landmarks = face_landmarks[::2]
            points = np.array([[lm['x'], lm['y'], lm['z']] for lm in sampled_landmarks])
            landmarks_3d['face'] = points

        # Extract pose landmarks with proper ordering
        if pose_data:
            # Define the complete MediaPipe pose landmark order
            mediapipe_pose_landmarks = [
                'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER',
                'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT',
                'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',
                'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX',
                'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP',
                'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL',
                'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'
            ]

            # Extract landmarks in the correct order with visibility check
            pose_points = []
            pose_landmark_names = []

            for landmark_name in mediapipe_pose_landmarks:
                if landmark_name in pose_data:
                    lm = pose_data[landmark_name]
                    # Check visibility if available
                    visibility = lm.get('visibility', 1.0)
                    if visibility > 0.5:  # Only include visible landmarks
                        pose_points.append([lm['x'], lm['y'], lm['z']])
                        pose_landmark_names.append(landmark_name)

            if pose_points:
                landmarks_3d['pose'] = np.array(pose_points)
                # Store the landmark names for connection drawing
                landmarks_3d['pose_names'] = pose_landmark_names

        return landmarks_3d

    def precompute_paths_from_json(self, frames_data: dict):
        """Precompute all hand paths from JSON data with advanced consistency correction and hand-wrist calibration"""
        print("Precomputing hand paths with advanced consistency correction and hand-wrist calibration...")

        # Sort frame keys numerically
        sorted_frame_keys = sorted(frames_data.keys(), key=int)

        # First pass: collect all hand data and apply consistency correction
        corrected_frames_data = {}

        print(f"Processing {len(sorted_frame_keys)} frames for hand consistency...")

        for i, frame_key in enumerate(sorted_frame_keys):
            frame_data = frames_data[frame_key].copy()
            hands_data = frame_data.get('hands', {})

            # Apply advanced consistency correction
            corrected_hands = self.consistency_tracker.correct_hand_labels(hands_data)

            # Update frame data with corrected hands
            frame_data['hands'] = corrected_hands
            corrected_frames_data[frame_key] = frame_data

            # Progress indicator
            if (i + 1) % 50 == 0 or i == len(sorted_frame_keys) - 1:
                print(f"  Processed {i + 1}/{len(sorted_frame_keys)} frames...")

        print("Building hand movement paths from corrected and calibrated data...")

        # Second pass: build paths from corrected data with hand-wrist calibration
        for frame_key in sorted_frame_keys:
            frame_data = corrected_frames_data[frame_key]
            hands_data = frame_data.get('hands', {})
            pose_data = frame_data.get('pose', {})

            # Extract pose wrist positions for calibration
            pose_wrists = {"LEFT_WRIST": None, "RIGHT_WRIST": None}
            if pose_data:
                if "LEFT_WRIST" in pose_data:
                    pose_wrists["LEFT_WRIST"] = np.array([
                        pose_data["LEFT_WRIST"]["x"],
                        pose_data["LEFT_WRIST"]["y"],
                        pose_data["LEFT_WRIST"]["z"]
                    ])

                if "RIGHT_WRIST" in pose_data:
                    pose_wrists["RIGHT_WRIST"] = np.array([
                        pose_data["RIGHT_WRIST"]["x"],
                        pose_data["RIGHT_WRIST"]["y"],
                        pose_data["RIGHT_WRIST"]["z"]
                    ])

            # FIRST: Swap the hands to correct perspective mismatch
            swapped_hands_data = {
                'left_hand': hands_data.get('right_hand', []),  # Swap right to left
                'right_hand': hands_data.get('left_hand', [])  # Swap left to right
            }

            # Now use normal mapping since we've already swapped
            hand_to_wrist_mapping = {
                'left_hand': 'LEFT_WRIST',
                'right_hand': 'RIGHT_WRIST'
            }

            # Process both hands with calibration
            for hand_type in ['left_hand', 'right_hand']:
                hand_data = swapped_hands_data.get(hand_type, [])
                wrist_name = hand_to_wrist_mapping[hand_type]
                pose_wrist_position = pose_wrists.get(wrist_name)

                if hand_data:
                    # Handle both old and new JSON formats
                    if isinstance(hand_data, dict) and 'landmarks' in hand_data:
                        landmarks = hand_data['landmarks']
                    else:
                        landmarks = hand_data

                    if landmarks and len(landmarks) > self.wrist_index:
                        wrist_point = landmarks[self.wrist_index]
                        hand_wrist_position = np.array([wrist_point['x'], wrist_point['y'], wrist_point['z']])

                        # Apply calibration if pose wrist is available
                        if pose_wrist_position is not None:
                            # Use pose wrist position instead of detected hand wrist
                            calibrated_position = pose_wrist_position
                        else:
                            # Fall back to detected hand wrist position
                            calibrated_position = hand_wrist_position

                        # Store the path point
                        path_data = {
                            'frame': int(frame_key),
                            'point': calibrated_position.tolist(),
                            'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0),
                            'calibrated': pose_wrist_position is not None
                        }

                        if hand_type == 'left_hand':
                            self.full_left_hand_path.append(path_data)
                        else:
                            self.full_right_hand_path.append(path_data)

        # Store corrected data for use in visualization
        self.corrected_frames_data = corrected_frames_data

        # Print correction statistics
        correction_stats = self.consistency_tracker.get_correction_stats()
        print(f"\nAdvanced Hand Consistency + Calibration Results:")
        print(f"  Total corrections applied: {correction_stats['corrections_made']}")
        print(f"  Correction rate: {correction_stats['correction_rate']:.1f}% of frames")
        print(f"  Distance threshold: {correction_stats['distance_threshold']}")
        print(f"  Confidence threshold: {correction_stats['confidence_threshold']} frames")

        # Count calibrated frames
        calibrated_left = sum(1 for p in self.full_left_hand_path if p.get('calibrated', False))
        calibrated_right = sum(1 for p in self.full_right_hand_path if p.get('calibrated', False))

        print(
            f"Final paths: Left hand {len(self.full_left_hand_path)} points ({calibrated_left} calibrated), Right hand {len(self.full_right_hand_path)} points ({calibrated_right} calibrated)")

        # Validate path consistency
        self._validate_path_consistency()

    def _draw_pose_connections(self, points: np.ndarray, pose_names: list = None):
        """Draw connections between pose landmarks using MediaPipe's pose structure"""
        if len(points) == 0:
            return

        # Define MediaPipe pose connections (index pairs)
        # These are the standard connections used by MediaPipe
        mediapipe_pose_connections = [
            # Face connections
            ('NOSE', 'LEFT_EYE_INNER'), ('LEFT_EYE_INNER', 'LEFT_EYE'),
            ('LEFT_EYE', 'LEFT_EYE_OUTER'), ('LEFT_EYE_OUTER', 'LEFT_EAR'),
            ('NOSE', 'RIGHT_EYE_INNER'), ('RIGHT_EYE_INNER', 'RIGHT_EYE'),
            ('RIGHT_EYE', 'RIGHT_EYE_OUTER'), ('RIGHT_EYE_OUTER', 'RIGHT_EAR'),
            ('MOUTH_LEFT', 'MOUTH_RIGHT'),

            # Torso connections
            ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
            ('LEFT_SHOULDER', 'LEFT_HIP'), ('RIGHT_SHOULDER', 'RIGHT_HIP'),
            ('LEFT_HIP', 'RIGHT_HIP'),

            # Left arm connections
            ('LEFT_SHOULDER', 'LEFT_ELBOW'), ('LEFT_ELBOW', 'LEFT_WRIST'),
            ('LEFT_WRIST', 'LEFT_PINKY'), ('LEFT_WRIST', 'LEFT_INDEX'),
            ('LEFT_WRIST', 'LEFT_THUMB'), ('LEFT_PINKY', 'LEFT_INDEX'),

            # Right arm connections
            ('RIGHT_SHOULDER', 'RIGHT_ELBOW'), ('RIGHT_ELBOW', 'RIGHT_WRIST'),
            ('RIGHT_WRIST', 'RIGHT_PINKY'), ('RIGHT_WRIST', 'RIGHT_INDEX'),
            ('RIGHT_WRIST', 'RIGHT_THUMB'), ('RIGHT_PINKY', 'RIGHT_INDEX'),

            # Left leg connections
            ('LEFT_HIP', 'LEFT_KNEE'), ('LEFT_KNEE', 'LEFT_ANKLE'),
            ('LEFT_ANKLE', 'LEFT_HEEL'), ('LEFT_ANKLE', 'LEFT_FOOT_INDEX'),
            ('LEFT_HEEL', 'LEFT_FOOT_INDEX'),

            # Right leg connections
            ('RIGHT_HIP', 'RIGHT_KNEE'), ('RIGHT_KNEE', 'RIGHT_ANKLE'),
            ('RIGHT_ANKLE', 'RIGHT_HEEL'), ('RIGHT_ANKLE', 'RIGHT_FOOT_INDEX'),
            ('RIGHT_HEEL', 'RIGHT_FOOT_INDEX'),
        ]

        # If we don't have pose names, fall back to simplified connections
        if pose_names is None or len(pose_names) != len(points):
            # Simplified fallback: connect major body parts if we have enough points
            if len(points) >= 4:
                # Try to connect what we assume are major landmarks
                color = self.colors['pose']
                # Connect first few points (assuming they're major landmarks)
                for i in range(min(4, len(points) - 1)):
                    self.ax.plot([points[i][0], points[i + 1][0]],
                                 [points[i][1], points[i + 1][1]],
                                 [points[i][2], points[i + 1][2]],
                                 color=color, alpha=0.6, linewidth=2)
            return

        # Create a mapping from landmark names to indices
        name_to_index = {name: i for i, name in enumerate(pose_names)}

        # Draw connections based on MediaPipe's structure
        color = self.colors['pose']
        connections_drawn = 0

        for start_name, end_name in mediapipe_pose_connections:
            if start_name in name_to_index and end_name in name_to_index:
                start_idx = name_to_index[start_name]
                end_idx = name_to_index[end_name]

                start_point = points[start_idx]
                end_point = points[end_idx]

                # Draw the connection
                self.ax.plot([start_point[0], end_point[0]],
                             [start_point[1], end_point[1]],
                             [start_point[2], end_point[2]],
                             color=color, alpha=0.7, linewidth=2)
                connections_drawn += 1

        # If no connections were drawn, fall back to simplified approach
        if connections_drawn == 0 and len(points) >= 2:
            print(f"Warning: No pose connections found, using simplified visualization")
            # Connect shoulders if we can identify them
            shoulder_connections = [
                ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
                ('LEFT_SHOULDER', 'LEFT_ELBOW'),
                ('RIGHT_SHOULDER', 'RIGHT_ELBOW')
            ]

            for start_name, end_name in shoulder_connections:
                if start_name in name_to_index and end_name in name_to_index:
                    start_idx = name_to_index[start_name]
                    end_idx = name_to_index[end_name]

                    start_point = points[start_idx]
                    end_point = points[end_idx]

                    self.ax.plot([start_point[0], end_point[0]],
                                 [start_point[1], end_point[1]],
                                 [start_point[2], end_point[2]],
                                 color=color, alpha=0.7, linewidth=2)

    def _draw_frame(self, frame_num: int):
        """Draw landmarks for a specific frame with improved pose visualization"""
        self.ax.clear()

        # Set up plot again
        self.ax.set_xlabel('X')
        self.ax.set_ylabel('Y')
        self.ax.set_zlabel('Z')
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 1)
        self.ax.set_zlim(-0.5, 0.5)
        self.ax.invert_yaxis()

        # Get frame key
        frame_keys = sorted(self.frames_data.keys(), key=int)
        if frame_num >= len(frame_keys):
            frame_num = len(frame_keys) - 1

        frame_key = frame_keys[frame_num]
        current_frame_number = int(frame_key)

        # Use corrected landmarks if hand tracking is enabled
        if self.hand_tracker:
            landmarks = self._extract_landmarks_for_frame_corrected(frame_key)
        else:
            landmarks = self._extract_landmarks_for_frame(frame_key)

        # Draw hand landmarks
        for hand_type, points in landmarks['hands'].items():
            if len(points) > 0:
                color = self.colors['hands'][hand_type]
                self.ax.scatter(points[:, 0], points[:, 1], points[:, 2],
                                c=color, s=30, alpha=0.8, label=f'{hand_type.replace("_", " ").title()}')

                # Draw hand connections (simplified)
                self._draw_hand_connections(points, color)

        # Draw face landmarks
        if len(landmarks['face']) > 0:
            self.ax.scatter(landmarks['face'][:, 0], landmarks['face'][:, 1], landmarks['face'][:, 2],
                            c=self.colors['face'], s=10, alpha=0.6, label='Face')

        # Draw pose landmarks with proper connections
        if len(landmarks['pose']) > 0:
            self.ax.scatter(landmarks['pose'][:, 0], landmarks['pose'][:, 1], landmarks['pose'][:, 2],
                            c=self.colors['pose'], s=50, alpha=0.8, label='Pose')

            # Draw pose connections using the landmark names if available
            pose_names = landmarks.get('pose_names', None)
            self._draw_pose_connections(landmarks['pose'], pose_names)

        # Draw hand paths if tracking is enabled
        if self.hand_tracker:
            # For animations, show progressive path up to current frame
            # For static, show full path
            show_full_path = not hasattr(self, '_is_animating') or not self._is_animating
            self.hand_tracker.draw_paths_3d(self.ax, current_frame_number, show_full_path)

        # Update title with frame info
        frame_timestamp = float(frame_key) / self.metadata.get('fps', 30)
        tracking_status = " (Hand Tracking + Consistency Fix)" if self.track_hands else ""
        self.ax.set_title(f'Frame {frame_key} (t={frame_timestamp:.2f}s){tracking_status}\n'
                          f'Source: {self.metadata.get("input_source", "Unknown")}')

        # Add legend
        self.ax.legend(loc='upper left', bbox_to_anchor=(0, 1))

        return self.ax
    def visualize_static(self, frame_number: Optional[int] = None):
        """Display a static 3D visualization of landmarks for a specific frame"""
        self._setup_3d_plot()

        # Mark as not animating for full path display
        self._is_animating = False

        if frame_number is None:
            frame_number = 0

        self._draw_frame(frame_number)

        # Add instructions with hand tracking info
        instruction_text = 'Use mouse to rotate view. Close window to exit.'
        if self.track_hands:
            correction_stats = self.hand_tracker.get_path_statistics()['consistency_corrections']
            instruction_text += f'\nShowing complete hand paths with {correction_stats["corrections_made"]} consistency corrections applied.'

        plt.figtext(0.02, 0.02, instruction_text, fontsize=10, style='italic')

        plt.tight_layout()
        plt.show()

    def visualize_animated(self, interval: float = 100):
        """Display an animated 3D visualization of landmarks across all frames"""
        self._setup_3d_plot()

        # Set animation flag for progressive path tracking
        self._is_animating = True

        frame_keys = sorted(self.frames_data.keys(), key=int)
        total_frames = len(frame_keys)

        def animate(frame):
            return self._draw_frame(frame)

        # Create animation
        self.anim = FuncAnimation(self.fig, animate, frames=total_frames,
                                  interval=interval, blit=False, repeat=True)

        # Add instructions
        instruction_text = 'Animation playing. Use mouse to rotate view. Close window to exit.'
        if self.track_hands:
            correction_stats = self.hand_tracker.get_path_statistics()['consistency_corrections']
            instruction_text += f'\nHand paths build progressively with {correction_stats["corrections_made"]} consistency corrections.'

        plt.figtext(0.02, 0.02, instruction_text, fontsize=10, style='italic')

        plt.tight_layout()
        plt.show()

        return self.anim

    def save_animation(self, output_path: str, fps: float = 10):
        """Save the 3D animation as a video file"""
        self._setup_3d_plot()

        frame_keys = sorted(self.frames_data.keys(), key=int)
        total_frames = len(frame_keys)

        def animate(frame):
            return self._draw_frame(frame)

        # Create animation
        anim = FuncAnimation(self.fig, animate, frames=total_frames,
                             interval=1000 / fps, blit=False, repeat=False)

        # Save animation
        print(f"Saving 3D animation to {output_path}...")
        anim.save(output_path, writer='pillow', fps=fps)
        print(f"Animation saved successfully!")

        plt.close()

    def analyze_hand_paths(self):
        """Analyze and print detailed hand path information"""
        if not self.hand_tracker:
            print("Hand tracking is not enabled. Use track_hands=True to enable analysis.")
            return

        stats = self.hand_tracker.get_path_statistics()
        correction_info = stats['consistency_corrections']

        print("\n" + "=" * 60)
        print("HAND PATH ANALYSIS (WITH CONSISTENCY CORRECTION)")
        print("=" * 60)

        print(f"\nVideo Information:")
        print(f"  Source: {self.metadata.get('input_source', 'Unknown')}")
        print(f"  FPS: {self.metadata.get('fps', 'Unknown')}")
        print(f"  Total frames: {len(self.frames_data)}")

        print(f"\nConsistency Correction:")
        print(f"  Left/Right hand swaps corrected: {correction_info['corrections_made']}")
        print(f"  Distance threshold used: {correction_info['distance_threshold']:.3f}")
        print(f"  Status: {'Applied' if correction_info['corrections_made'] > 0 else 'No corrections needed'}")

        print(f"\nLeft Hand Movement:")
        print(f"  Detected in {stats['left_hand']['total_points']} frames")
        if stats['left_hand']['total_points'] > 0:
            print(f"  Total distance traveled: {stats['left_hand']['total_distance']:.3f} units")
            print(f"  Average speed: {stats['left_hand']['avg_speed']:.3f} units/second")
            print(f"  Maximum speed: {stats['left_hand']['max_speed']:.3f} units/second")

        print(f"\nRight Hand Movement:")
        print(f"  Detected in {stats['right_hand']['total_points']} frames")
        if stats['right_hand']['total_points'] > 0:
            print(f"  Total distance traveled: {stats['right_hand']['total_distance']:.3f} units")
            print(f"  Average speed: {stats['right_hand']['avg_speed']:.3f} units/second")
            print(f"  Maximum speed: {stats['right_hand']['max_speed']:.3f} units/second")

        # Detect periods of high activity
        print(f"\nMovement Analysis:")
        if self.hand_tracker.full_left_hand_path or self.hand_tracker.full_right_hand_path:
            print("  Use the 3D visualization to see detailed movement patterns.")
            print("  Green markers = Start positions")
            print("  Red/Blue markers = Current/End positions")
            print("  Path opacity increases toward the end of movement")
            print("  Hand paths are corrected for left/right consistency")

        print("=" * 60)


class HandConsistencyTracker:
    """Advanced hand tracking to fix left/right hand swapping issues"""

    def __init__(self, distance_threshold: float = 0.2, confidence_threshold: int = 3):
        """
        Initialize hand consistency tracker

        Args:
            distance_threshold: Maximum distance for hand to be considered the same hand
            confidence_threshold: Number of consecutive frames needed to confirm a swap
        """
        self.distance_threshold = distance_threshold
        self.confidence_threshold = confidence_threshold

        # Track hand positions over multiple frames
        self.hand_history = {
            'left_hand': [],
            'right_hand': []
        }

        # Keep track of potential swaps
        self.swap_confidence = 0
        self.pending_swap = False

        # Statistics
        self.correction_count = 0
        self.frame_count = 0

        # Smoothing parameters
        self.max_history = 5  # Keep last 5 positions for smoothing

    def _extract_hand_position(self, hand_data):
        """Extract wrist position from hand data"""
        if not hand_data:
            return None

        if isinstance(hand_data, dict) and 'landmarks' in hand_data:
            landmarks = hand_data['landmarks']
        else:
            landmarks = hand_data

        if landmarks and len(landmarks) > 0:
            wrist = landmarks[0]
            return np.array([wrist['x'], wrist['y'], wrist['z']])
        return None

    def _get_average_position(self, hand_type):
        """Get average position from recent history"""
        if not self.hand_history[hand_type]:
            return None

        positions = self.hand_history[hand_type][-3:]  # Use last 3 positions
        return np.mean(positions, axis=0)

    def _should_swap_hands(self, current_left_pos, current_right_pos):
        """Determine if hands should be swapped based on multiple criteria"""
        if current_left_pos is None or current_right_pos is None:
            return False

        # Get expected positions based on history
        expected_left = self._get_average_position('left_hand')
        expected_right = self._get_average_position('right_hand')

        if expected_left is None or expected_right is None:
            return False

        # Calculate distances
        left_to_expected_left = np.linalg.norm(current_left_pos - expected_left)
        left_to_expected_right = np.linalg.norm(current_left_pos - expected_right)
        right_to_expected_left = np.linalg.norm(current_right_pos - expected_left)
        right_to_expected_right = np.linalg.norm(current_right_pos - expected_right)

        # Check if current labels are swapped
        current_assignment_cost = left_to_expected_left + right_to_expected_right
        swapped_assignment_cost = left_to_expected_right + right_to_expected_left

        # Additional criteria: X-position consistency (left hand should generally be on the left side)
        x_position_consistent = current_left_pos[0] <= current_right_pos[0]  # Left should be more to the left

        # Decide if swap is needed
        swap_needed = (
                swapped_assignment_cost < current_assignment_cost and
                swapped_assignment_cost < self.distance_threshold * 2 and
                not x_position_consistent
        )

        return swap_needed

    def correct_hand_labels(self, hands_data: dict):
        """
        Correct hand labels using advanced tracking with confidence voting

        Args:
            hands_data: Dictionary with 'left_hand' and 'right_hand' data

        Returns:
            Corrected hands_data dictionary
        """
        self.frame_count += 1

        # Extract current positions
        current_left_pos = self._extract_hand_position(hands_data.get('left_hand', []))
        current_right_pos = self._extract_hand_position(hands_data.get('right_hand', []))

        # If we don't have both hands, just update history and return
        if current_left_pos is None or current_right_pos is None:
            # Update history for available hands
            if current_left_pos is not None:
                self.hand_history['left_hand'].append(current_left_pos)
                if len(self.hand_history['left_hand']) > self.max_history:
                    self.hand_history['left_hand'].pop(0)

            if current_right_pos is not None:
                self.hand_history['right_hand'].append(current_right_pos)
                if len(self.hand_history['right_hand']) > self.max_history:
                    self.hand_history['right_hand'].pop(0)

            # Reset swap confidence when we don't have both hands
            self.swap_confidence = 0
            self.pending_swap = False
            return hands_data

        # For the first few frames, just build up history
        if len(self.hand_history['left_hand']) < 2:
            self.hand_history['left_hand'].append(current_left_pos)
            self.hand_history['right_hand'].append(current_right_pos)
            return hands_data

        # Check if swap is needed
        swap_needed = self._should_swap_hands(current_left_pos, current_right_pos)

        # Update confidence counter
        if swap_needed:
            self.swap_confidence += 1
        else:
            self.swap_confidence = max(0, self.swap_confidence - 1)

        # Apply swap if confidence threshold is met
        if self.swap_confidence >= self.confidence_threshold:
            corrected_hands_data = {
                'left_hand': hands_data.get('right_hand', []),
                'right_hand': hands_data.get('left_hand', [])
            }

            # Update history with swapped positions
            self.hand_history['left_hand'].append(current_right_pos)
            self.hand_history['right_hand'].append(current_left_pos)

            self.correction_count += 1
            self.swap_confidence = 0  # Reset after correction

            # Trim history
            if len(self.hand_history['left_hand']) > self.max_history:
                self.hand_history['left_hand'].pop(0)
            if len(self.hand_history['right_hand']) > self.max_history:
                self.hand_history['right_hand'].pop(0)

            return corrected_hands_data
        else:
            # Update history with current positions
            self.hand_history['left_hand'].append(current_left_pos)
            self.hand_history['right_hand'].append(current_right_pos)

            # Trim history
            if len(self.hand_history['left_hand']) > self.max_history:
                self.hand_history['left_hand'].pop(0)
            if len(self.hand_history['right_hand']) > self.max_history:
                self.hand_history['right_hand'].pop(0)

            return hands_data

    def get_correction_stats(self):
        """Get statistics about corrections made"""
        return {
            'corrections_made': self.correction_count,
            'total_frames': self.frame_count,
            'correction_rate': self.correction_count / max(1, self.frame_count) * 100,
            'distance_threshold': self.distance_threshold,
            'confidence_threshold': self.confidence_threshold
        }


class HandPathTracker:
    """Tracks and visualizes hand movement paths from JSON data with advanced consistency correction"""

    def __init__(self, max_path_length: int = 50):
        """
        Initialize hand path tracker

        Args:
            max_path_length: Maximum number of points to keep in path history
        """
        self.max_path_length = max_path_length
        self.left_hand_path = deque(maxlen=max_path_length)
        self.right_hand_path = deque(maxlen=max_path_length)

        # Store full paths for analysis
        self.full_left_hand_path = []
        self.full_right_hand_path = []

        # Initialize advanced consistency tracker
        self.consistency_tracker = HandConsistencyTracker(
            distance_threshold=0.15,  # Stricter threshold
            confidence_threshold=2  # Require 2 consecutive frames before swapping
        )

        # Path colors (slightly transparent)
        self.path_colors = {
            'left_hand': (1.0, 0.5, 0.5, 0.7),  # Light red with alpha
            'right_hand': (0.5, 0.5, 1.0, 0.7)  # Light blue with alpha
        }

        # Wrist landmark index (MediaPipe hand landmark 0 is wrist)
        self.wrist_index = 0

    def precompute_paths_from_json(self, frames_data: dict):
        """Precompute all hand paths from JSON data with advanced consistency correction"""
        print("Precomputing hand paths with advanced consistency correction...")

        # Sort frame keys numerically
        sorted_frame_keys = sorted(frames_data.keys(), key=int)

        # First pass: collect all hand data and apply consistency correction
        corrected_frames_data = {}

        print(f"Processing {len(sorted_frame_keys)} frames for hand consistency...")

        for i, frame_key in enumerate(sorted_frame_keys):
            frame_data = frames_data[frame_key].copy()
            hands_data = frame_data.get('hands', {})

            # Apply advanced consistency correction
            corrected_hands = self.consistency_tracker.correct_hand_labels(hands_data)

            # Update frame data with corrected hands
            frame_data['hands'] = corrected_hands
            corrected_frames_data[frame_key] = frame_data

            # Progress indicator
            if (i + 1) % 50 == 0 or i == len(sorted_frame_keys) - 1:
                print(f"  Processed {i + 1}/{len(sorted_frame_keys)} frames...")

        print("Building hand movement paths from corrected data...")

        # Second pass: build paths from corrected data
        for frame_key in sorted_frame_keys:
            frame_data = corrected_frames_data[frame_key]
            hands_data = frame_data.get('hands', {})

            # Process left hand
            left_hand = hands_data.get('left_hand', [])
            if left_hand:
                # Handle both old and new JSON formats
                if isinstance(left_hand, dict) and 'landmarks' in left_hand:
                    landmarks = left_hand['landmarks']
                else:
                    landmarks = left_hand

                if landmarks and len(landmarks) > self.wrist_index:
                    wrist_point = landmarks[self.wrist_index]
                    point = [wrist_point['x'], wrist_point['y'], wrist_point['z']]
                    self.full_left_hand_path.append({
                        'frame': int(frame_key),
                        'point': point,
                        'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0)
                    })

            # Process right hand
            right_hand = hands_data.get('right_hand', [])
            if right_hand:
                # Handle both old and new JSON formats
                if isinstance(right_hand, dict) and 'landmarks' in right_hand:
                    landmarks = right_hand['landmarks']
                else:
                    landmarks = right_hand

                if landmarks and len(landmarks) > self.wrist_index:
                    wrist_point = landmarks[self.wrist_index]
                    point = [wrist_point['x'], wrist_point['y'], wrist_point['z']]
                    self.full_right_hand_path.append({
                        'frame': int(frame_key),
                        'point': point,
                        'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0)
                    })

        # Store corrected data for use in visualization
        self.corrected_frames_data = corrected_frames_data

        # Print correction statistics
        correction_stats = self.consistency_tracker.get_correction_stats()
        print(f"\nAdvanced Hand Consistency Results:")
        print(f"  Total corrections applied: {correction_stats['corrections_made']}")
        print(f"  Correction rate: {correction_stats['correction_rate']:.1f}% of frames")
        print(f"  Distance threshold: {correction_stats['distance_threshold']}")
        print(f"  Confidence threshold: {correction_stats['confidence_threshold']} frames")
        print(
            f"Final paths: Left hand {len(self.full_left_hand_path)} points, Right hand {len(self.full_right_hand_path)} points")

        # Validate path consistency
        self._validate_path_consistency()

    def _validate_path_consistency(self):
        """Validate that the paths look consistent (no major jumps)"""
        print("\nValidating path consistency...")

        def check_path_jumps(path_data, hand_name):
            if len(path_data) < 2:
                return 0

            jump_count = 0
            jump_threshold = 0.3  # Large movement threshold

            for i in range(1, len(path_data)):
                p1 = np.array(path_data[i - 1]['point'])
                p2 = np.array(path_data[i]['point'])
                distance = np.linalg.norm(p2 - p1)

                if distance > jump_threshold:
                    jump_count += 1

            return jump_count

        left_jumps = check_path_jumps(self.full_left_hand_path, "Left")
        right_jumps = check_path_jumps(self.full_right_hand_path, "Right")

        print(f"  Left hand large movements: {left_jumps}")
        print(f"  Right hand large movements: {right_jumps}")

        if left_jumps + right_jumps < 5:
            print("   Paths look consistent!")
        else:
            print("    Some large movements detected - this might be normal or indicate remaining issues")

    def get_corrected_frame_data(self, frame_key: str):
        """Get corrected frame data for a specific frame"""
        return self.corrected_frames_data.get(frame_key, {})

    def update_paths_for_frame(self, target_frame: int):
        """Update visible paths up to the target frame"""
        self.left_hand_path.clear()
        self.right_hand_path.clear()

        # Add left hand points up to target frame
        for path_point in self.full_left_hand_path:
            if path_point['frame'] <= target_frame:
                self.left_hand_path.append(path_point['point'])
            else:
                break

        # Add right hand points up to target frame
        for path_point in self.full_right_hand_path:
            if path_point['frame'] <= target_frame:
                self.right_hand_path.append(path_point['point'])
            else:
                break

    def get_path_for_frame_range(self, start_frame: int, end_frame: int, hand_type: str):
        """Get path points for a specific frame range"""
        if hand_type == 'left_hand':
            path_data = self.full_left_hand_path
        elif hand_type == 'right_hand':
            path_data = self.full_right_hand_path
        else:
            return []

        return [point['point'] for point in path_data
                if start_frame <= point['frame'] <= end_frame]

    def draw_paths_3d(self, ax, current_frame: int = None, show_full_path: bool = False):
        """Draw hand paths on 3D plot"""
        if show_full_path or current_frame is None:
            # Draw complete paths
            left_points = [p['point'] for p in self.full_left_hand_path]
            right_points = [p['point'] for p in self.full_right_hand_path]
        else:
            # Draw paths up to current frame
            left_points = [p['point'] for p in self.full_left_hand_path if p['frame'] <= current_frame]
            right_points = [p['point'] for p in self.full_right_hand_path if p['frame'] <= current_frame]

        # Draw left hand path
        if len(left_points) > 1:
            path_array = np.array(left_points)

            # Draw main path line
            ax.plot(path_array[:, 0], path_array[:, 1], path_array[:, 2],
                    color=self.path_colors['left_hand'][:3],
                    alpha=self.path_colors['left_hand'][3],
                    linewidth=3, label='Left Hand Path (Advanced Corrected)')

            # Add gradient effect by plotting segments with varying alpha
            num_segments = min(20, len(path_array) - 1)
            if num_segments > 0:
                segment_size = max(1, len(path_array) // num_segments)

                for i in range(0, len(path_array) - segment_size, segment_size):
                    end_idx = min(i + segment_size, len(path_array))
                    alpha = (i / len(path_array)) * 0.3 + 0.4  # Alpha from 0.4 to 0.7
                    ax.plot(path_array[i:end_idx, 0], path_array[i:end_idx, 1], path_array[i:end_idx, 2],
                            color=self.path_colors['left_hand'][:3], alpha=alpha, linewidth=2)

            # Mark start and end points
            if len(path_array) > 0:
                # Start point (green)
                ax.scatter([path_array[0, 0]], [path_array[0, 1]], [path_array[0, 2]],
                           c='green', s=100, marker='o', alpha=0.8, label='Left Start')
                # End point (red)
                ax.scatter([path_array[-1, 0]], [path_array[-1, 1]], [path_array[-1, 2]],
                           c='red', s=100, marker='s', alpha=0.8, label='Left Current')

        # Draw right hand path
        if len(right_points) > 1:
            path_array = np.array(right_points)

            # Draw main path line
            ax.plot(path_array[:, 0], path_array[:, 1], path_array[:, 2],
                    color=self.path_colors['right_hand'][:3],
                    alpha=self.path_colors['right_hand'][3],
                    linewidth=3, label='Right Hand Path (Advanced Corrected)')

            # Add gradient effect
            num_segments = min(20, len(path_array) - 1)
            if num_segments > 0:
                segment_size = max(1, len(path_array) // num_segments)

                for i in range(0, len(path_array) - segment_size, segment_size):
                    end_idx = min(i + segment_size, len(path_array))
                    alpha = (i / len(path_array)) * 0.3 + 0.4  # Alpha from 0.4 to 0.7
                    ax.plot(path_array[i:end_idx, 0], path_array[i:end_idx, 1], path_array[i:end_idx, 2],
                            color=self.path_colors['right_hand'][:3], alpha=alpha, linewidth=2)

            # Mark start and end points
            if len(path_array) > 0:
                # Start point (green)
                ax.scatter([path_array[0, 0]], [path_array[0, 1]], [path_array[0, 2]],
                           c='green', s=100, marker='o', alpha=0.8, label='Right Start')
                # End point (blue)
                ax.scatter([path_array[-1, 0]], [path_array[-1, 1]], [path_array[-1, 2]],
                           c='blue', s=100, marker='s', alpha=0.8, label='Right Current')

    def get_path_statistics(self):
        """Get statistics about hand movements"""
        stats = {
            'left_hand': {
                'total_points': len(self.full_left_hand_path),
                'total_distance': 0.0,
                'avg_speed': 0.0,
                'max_speed': 0.0
            },
            'right_hand': {
                'total_points': len(self.full_right_hand_path),
                'total_distance': 0.0,
                'avg_speed': 0.0,
                'max_speed': 0.0
            },
            'consistency_corrections': self.consistency_tracker.get_correction_stats()
        }

        # Calculate distances and speeds for left hand
        if len(self.full_left_hand_path) > 1:
            distances = []
            speeds = []
            for i in range(1, len(self.full_left_hand_path)):
                p1 = np.array(self.full_left_hand_path[i - 1]['point'])
                p2 = np.array(self.full_left_hand_path[i]['point'])
                distance = np.linalg.norm(p2 - p1)
                distances.append(distance)

                time_diff = self.full_left_hand_path[i]['timestamp'] - self.full_left_hand_path[i - 1]['timestamp']
                if time_diff > 0:
                    speed = distance / time_diff
                    speeds.append(speed)

            stats['left_hand']['total_distance'] = sum(distances)
            if speeds:
                stats['left_hand']['avg_speed'] = np.mean(speeds)
                stats['left_hand']['max_speed'] = np.max(speeds)

        # Calculate distances and speeds for right hand
        if len(self.full_right_hand_path) > 1:
            distances = []
            speeds = []
            for i in range(1, len(self.full_right_hand_path)):
                p1 = np.array(self.full_right_hand_path[i - 1]['point'])
                p2 = np.array(self.full_right_hand_path[i]['point'])
                distance = np.linalg.norm(p2 - p1)
                distances.append(distance)

                time_diff = self.full_right_hand_path[i]['timestamp'] - self.full_right_hand_path[i - 1]['timestamp']
                if time_diff > 0:
                    speed = distance / time_diff
                    speeds.append(speed)

            stats['right_hand']['total_distance'] = sum(distances)
            if speeds:
                stats['right_hand']['avg_speed'] = np.mean(speeds)
                stats['right_hand']['max_speed'] = np.max(speeds)

        return stats


def generate_2d_path_overlay_from_json(json_path: str, output_video_path: str = None):
    """
    Generate a 2D video with hand path overlays from JSON data with consistency correction
    This works independently of the original video processing
    """
    # Load JSON data
    with open(json_path, 'r') as f:
        data = json.load(f)

    frames_data = data.get('frames', {})
    metadata = data.get('metadata', {})

    if not frames_data:
        print("No frame data found in JSON")
        return

    # Get video properties from metadata
    fps = metadata.get('fps', 30)
    resolution = metadata.get('resolution', '640x480')
    width, height = map(int, resolution.split('x'))

    # Set up output video
    if output_video_path is None:
        output_video_path = Path(json_path).parent / "hand_paths_overlay_corrected.mp4"

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))

    # Initialize hand tracker with consistency correction
    hand_tracker = HandPathTracker()
    hand_tracker.precompute_paths_from_json(frames_data)

    # Generate frames
    frame_keys = sorted(frames_data.keys(), key=int)

    for frame_key in frame_keys:
        # Create blank frame
        frame = np.zeros((height, width, 3), dtype=np.uint8)

        current_frame_num = int(frame_key)

        # Get paths up to current frame
        left_points = [(int(p['point'][0] * width), int(p['point'][1] * height))
                       for p in hand_tracker.full_left_hand_path
                       if p['frame'] <= current_frame_num]

        right_points = [(int(p['point'][0] * width), int(p['point'][1] * height))
                        for p in hand_tracker.full_right_hand_path
                        if p['frame'] <= current_frame_num]

        # Draw paths with gradient effect
        if len(left_points) > 1:
            for i in range(1, len(left_points)):
                alpha = i / len(left_points)
                thickness = max(1, int(5 * alpha))
                cv2.line(frame, left_points[i - 1], left_points[i], (0, 100, 255), thickness)

        if len(right_points) > 1:
            for i in range(1, len(right_points)):
                alpha = i / len(right_points)
                thickness = max(1, int(5 * alpha))
                cv2.line(frame, right_points[i - 1], right_points[i], (255, 100, 0), thickness)

        # Add current hand positions using corrected data
        corrected_frame_data = hand_tracker.get_corrected_frame_data(frame_key)
        hands_data = corrected_frame_data.get('hands', {})

        for hand_type, color in [('left_hand', (0, 255, 255)), ('right_hand', (255, 255, 0))]:
            hand_data = hands_data.get(hand_type, [])
            if hand_data:
                if isinstance(hand_data, dict) and 'landmarks' in hand_data:
                    landmarks = hand_data['landmarks']
                else:
                    landmarks = hand_data

                if landmarks:
                    wrist = landmarks[0]
                    x, y = int(wrist['x'] * width), int(wrist['y'] * height)
                    cv2.circle(frame, (x, y), 8, color, -1)
                    cv2.putText(frame, hand_type.replace('_', ' ').title(),
                                (x + 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

        # Add frame info and correction status
        correction_stats = hand_tracker.get_path_statistics()['consistency_corrections']
        cv2.putText(frame, f"Frame: {frame_key}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
        cv2.putText(frame, f"Corrections: {correction_stats['corrections_made']}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        out.write(frame)

    out.release()
    correction_stats = hand_tracker.get_path_statistics()['consistency_corrections']
    print(f"Hand path overlay video saved to: {output_video_path}")
    print(f"Applied {correction_stats['corrections_made']} consistency corrections")


def visualize_landmarks_3d(json_path: str, mode: str = 'static', frame_number: Optional[int] = None,
                           save_path: Optional[str] = None, track_hands: bool = False):
    """
    Visualize landmarks from video_landmarks.json in 3D space

    Args:
        json_path: Path to the video_landmarks.json file
        mode: 'static' for single frame, 'animated' for animation
        frame_number: Specific frame to show (for static mode), None for first frame
        save_path: Path to save animation (optional, for animated mode)
        track_hands: Whether to enable hand path tracking with consistency correction
    """
    try:
        visualizer = LandmarksVisualizer3D(json_path, track_hands=track_hands)

        # Show analysis if hand tracking is enabled
        if track_hands:
            visualizer.analyze_hand_paths()

        if mode == 'static':
            visualizer.visualize_static(frame_number)
        elif mode == 'animated':
            if save_path:
                visualizer.save_animation(save_path)
            else:
                visualizer.visualize_animated()
        else:
            raise ValueError("Mode must be 'static' or 'animated'")

    except Exception as e:
        print(f"Error in visualization: {e}")
        raise


def main():
    """Command line interface for 3D visualization"""
    parser = argparse.ArgumentParser(description='Visualize sign language landmarks in 3D')
    parser.add_argument('json_path', help='Path to video_landmarks.json file')
    parser.add_argument('--mode', choices=['static', 'animated'], default='static',
                        help='Visualization mode (default: static)')
    parser.add_argument('--frame', type=int, default=0,
                        help='Frame number to display (for static mode, default: 0)')
    parser.add_argument('--save', type=str,
                        help='Save animation to file (for animated mode)')
    parser.add_argument('--track-hands', action='store_true',
                        help='Enable hand path visualization with left/right consistency correction')

    args = parser.parse_args()

    print(f"Loading landmarks from: {args.json_path}")
    print(f"Mode: {args.mode}")
    print(f"Hand tracking: {'ON' if args.track_hands else 'OFF'}")

    if args.mode == 'static':
        print(f"Displaying frame: {args.frame}")

    visualize_landmarks_3d(args.json_path, args.mode, args.frame, args.save, args.track_hands)


if __name__ == "__main__":
    main()