"""
3D Landmarks Visualizer

This module provides functionality to visualize sign language landmarks in 3D space
from video_landmarks.json files generated by the processing pipeline.

Usage:
    from visualizer_3d import visualize_landmarks_3d
    visualize_landmarks_3d("path/to/video_landmarks.json")
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from pathlib import Path
from typing import Dict, Optional
import argparse
from collections import deque
import cv2

# TODO move hand-to-wrist and face-to-nose calibration to processing (with an option to not do it and leave it only for display if possible)
# TODO make sure that visualization happens from json for video (% sure it does)
# TODO when do we apply mirroring? before saving to json or at visualization? is it necessary or just viewing flavor?

FACE_AMPLIFICATION = 6
HANDS_AMPLIFICATION = 7
BOX_ASPECT = [1, 1, 1]  # to squash/str ech axes by a factor
DEBUG_HAND_DEPTH = False


class LandmarksVisualizer3D:
    """3D visualizer for sign language landmarks"""

    def __init__(self, json_path: str, frame_rate: float = 10.0, track_hands: bool = False):
        """
        Initialize the 3D visualizer

        Args:
            json_path: Path to video_landmarks.json file
            frame_rate: Animation frame rate (frames per second)
            track_hands: Whether to enable hand path tracking
        """
        self.json_path = Path(json_path)
        self.frame_rate = frame_rate
        self.track_hands = track_hands
        self.data = None
        self.frames_data = None
        self.metadata = None
        self.current_frame = 0

        # Initialize hand path tracker if enabled
        self.hand_tracker = None

        # Color scheme for different landmark types
        self.colors = {
            'hands': {'left_hand': 'red', 'right_hand': 'blue'},
            'face': 'orange',
            'pose': 'green'
        }

        # Load data
        self._load_data()

        # Initialize hand tracking after data is loaded
        if track_hands:
            self.hand_tracker = HandPathTracker()
            self.hand_tracker.precompute_paths_from_json(self.frames_data)

            # Print path statistics
            stats = self.hand_tracker.get_path_statistics()
            correction_info = stats['consistency_corrections']
            print("\nHand Path Statistics (with advanced consistency correction):")
            print(f"  Advanced corrections applied: {correction_info['corrections_made']}")
            print(f"  Correction rate: {correction_info['correction_rate']:.1f}% of frames")
            print(f"  Left hand: {stats['left_hand']['total_points']} points, "
                  f"distance: {stats['left_hand']['total_distance']:.3f}, "
                  f"avg speed: {stats['left_hand']['avg_speed']:.3f}")
            print(f"  Right hand: {stats['right_hand']['total_points']} points, "
                  f"distance: {stats['right_hand']['total_distance']:.3f}, "
                  f"avg speed: {stats['right_hand']['avg_speed']:.3f}")

    def _load_data(self):
        """Load landmarks data from JSON file"""
        if not self.json_path.exists():
            raise FileNotFoundError(f"JSON file not found: {self.json_path}")

        try:
            with open(self.json_path, 'r') as f:
                self.data = json.load(f)

            self.metadata = self.data.get('metadata', {})
            self.frames_data = self.data.get('frames', {})

            if not self.frames_data:
                raise ValueError("No frames data found in JSON file")

            print(f"Loaded data for {len(self.frames_data)} frames")
            print(f"Source: {self.metadata.get('input_source', 'Unknown')}")
            print(f"FPS: {self.metadata.get('fps', 'Unknown')}")

        except Exception as e:
            raise Exception(f"Error loading JSON file: {e}")

    def _setup_3d_plot(self):
        """Set up the 3D matplotlib plot"""
        self.fig = plt.figure(figsize=(12, 9))
        self.ax = self.fig.add_subplot(111, projection='3d')

        # Set labels and title
        self.ax.set_xlabel('X')
        self.ax.set_ylabel('Y')
        self.ax.set_zlabel('Z')
        self.ax.set_title(f'3D Landmarks Visualization\nSource: {self.metadata.get("input_source", "Unknown")}')

        # Set axis limits (normalized coordinates are 0-1)
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 1)
        self.ax.set_zlim(-2, 2)  # Larger Z range for amplified depth

        # Set viewing angle
        self.ax.set_box_aspect([1, 1, 1])
        self.ax.view_init(elev=-90, azim=90)

        return self.ax

    def _draw_hand_connections(self, points: np.ndarray, color: str):
        """Draw connections between hand landmarks"""
        if len(points) != 21:  # MediaPipe hand has 21 landmarks
            return

        # Define hand connections (simplified version)
        connections = [
            # Thumb
            (0, 1), (1, 2), (2, 3), (3, 4),
            # Index finger
            (0, 5), (5, 6), (6, 7), (7, 8),
            # Middle finger
            (0, 9), (9, 10), (10, 11), (11, 12),
            # Ring finger
            (0, 13), (13, 14), (14, 15), (15, 16),
            # Pinky
            (0, 17), (17, 18), (18, 19), (19, 20),
            # Palm connections
            (5, 9), (9, 13), (13, 17)
        ]

        for start_idx, end_idx in connections:
            start_point = points[start_idx]
            end_point = points[end_idx]
            self.ax.plot([start_point[0], end_point[0]],
                         [start_point[1], end_point[1]],
                         [start_point[2], end_point[2]],
                         color=color, alpha=0.5, linewidth=1)

    def _draw_pose_connections(self, points: np.ndarray, pose_names: list = None):
        """Draw connections between pose landmarks using MediaPipe's pose structure"""
        if len(points) == 0:
            return

        # Define MediaPipe pose connections (index pairs)
        # These are the standard connections used by MediaPipe
        mediapipe_pose_connections = [
            # Face connections
            # ('NOSE', 'LEFT_EYE_INNER'), ('LEFT_EYE_INNER', 'LEFT_EYE'),
            # ('LEFT_EYE', 'LEFT_EYE_OUTER'), ('LEFT_EYE_OUTER', 'LEFT_EAR'),
            # ('NOSE', 'RIGHT_EYE_INNER'), ('RIGHT_EYE_INNER', 'RIGHT_EYE'),
            # ('RIGHT_EYE', 'RIGHT_EYE_OUTER'), ('RIGHT_EYE_OUTER', 'RIGHT_EAR'),
            # ('MOUTH_LEFT', 'MOUTH_RIGHT'),

            # Torso connections
            ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
            ('LEFT_SHOULDER', 'LEFT_HIP'), ('RIGHT_SHOULDER', 'RIGHT_HIP'),
            ('LEFT_HIP', 'RIGHT_HIP'),

            # Left arm connections
            ('LEFT_SHOULDER', 'LEFT_ELBOW'), ('LEFT_ELBOW', 'LEFT_WRIST'),
            # ('LEFT_WRIST', 'LEFT_PINKY'), ('LEFT_WRIST', 'LEFT_INDEX'),
            # ('LEFT_WRIST', 'LEFT_THUMB'), ('LEFT_PINKY', 'LEFT_INDEX'),

            # Right arm connections
            ('RIGHT_SHOULDER', 'RIGHT_ELBOW'), ('RIGHT_ELBOW', 'RIGHT_WRIST'),
            # ('RIGHT_WRIST', 'RIGHT_PINKY'), ('RIGHT_WRIST', 'RIGHT_INDEX'),
            # ('RIGHT_WRIST', 'RIGHT_THUMB'), ('RIGHT_PINKY', 'RIGHT_INDEX'),

            # Left leg connections
            # ('LEFT_HIP', 'LEFT_KNEE'), ('LEFT_KNEE', 'LEFT_ANKLE'),
            # ('LEFT_ANKLE', 'LEFT_HEEL'), ('LEFT_ANKLE', 'LEFT_FOOT_INDEX'),
            # ('LEFT_HEEL', 'LEFT_FOOT_INDEX'),
            #
            # # Right leg connections
            # ('RIGHT_HIP', 'RIGHT_KNEE'), ('RIGHT_KNEE', 'RIGHT_ANKLE'),
            # ('RIGHT_ANKLE', 'RIGHT_HEEL'), ('RIGHT_ANKLE', 'RIGHT_FOOT_INDEX'),
            # ('RIGHT_HEEL', 'RIGHT_FOOT_INDEX'),
        ]

        # If we don't have pose names, fall back to simplified connections
        if pose_names is None or len(pose_names) != len(points):
            # Simplified fallback: connect major body parts if we have enough points
            if len(points) >= 4:
                # Try to connect what we assume are major landmarks
                color = self.colors['pose']
                # Connect first few points (assuming they're major landmarks)
                for i in range(min(4, len(points) - 1)):
                    self.ax.plot([points[i][0], points[i + 1][0]],
                                 [points[i][1], points[i + 1][1]],
                                 [points[i][2], points[i + 1][2]],
                                 color=color, alpha=0.6, linewidth=2)
            return

        # Create a mapping from landmark names to indices
        name_to_index = {name: i for i, name in enumerate(pose_names)}

        # Draw connections based on MediaPipe's structure
        color = self.colors['pose']
        connections_drawn = 0

        for start_name, end_name in mediapipe_pose_connections:
            if start_name in name_to_index and end_name in name_to_index:
                start_idx = name_to_index[start_name]
                end_idx = name_to_index[end_name]

                start_point = points[start_idx]
                end_point = points[end_idx]

                # Draw the connection
                self.ax.plot([start_point[0], end_point[0]],
                             [start_point[1], end_point[1]],
                             [start_point[2], end_point[2]],
                             color=color, alpha=0.7, linewidth=2)
                connections_drawn += 1

        # If no connections were drawn, fall back to simplified approach
        if connections_drawn == 0 and len(points) >= 2:
            print(f"Warning: No pose connections found, using simplified visualization")
            # Connect shoulders if we can identify them
            shoulder_connections = [
                ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
                ('LEFT_SHOULDER', 'LEFT_ELBOW'),
                ('RIGHT_SHOULDER', 'RIGHT_ELBOW')
            ]

            for start_name, end_name in shoulder_connections:
                if start_name in name_to_index and end_name in name_to_index:
                    start_idx = name_to_index[start_name]
                    end_idx = name_to_index[end_name]

                    start_point = points[start_idx]
                    end_point = points[end_idx]

                    self.ax.plot([start_point[0], end_point[0]],
                                 [start_point[1], end_point[1]],
                                 [start_point[2], end_point[2]],
                                 color=color, alpha=0.7, linewidth=2)

    # def _apply_mirroring_to_landmarks(self, landmarks_3d: Dict) -> Dict:
    #     """
    #     Helper function to apply consistent horizontal mirroring to all landmark types
    #
    #     Args:
    #         landmarks_3d: Dictionary containing landmarks for hands, face, and pose
    #
    #     Returns:
    #         Dictionary with mirrored landmarks
    #     """
    #     mirrored_landmarks = {
    #         'hands': {'left_hand': [], 'right_hand': []},
    #         'face': [],
    #         'pose': []
    #     }
    #
    #     # Mirror hand landmarks
    #     for hand_type in ['left_hand', 'right_hand']:
    #         if len(landmarks_3d['hands'][hand_type]) > 0:
    #             points = landmarks_3d['hands'][hand_type].copy()
    #             # Mirror X coordinates (flip horizontally)
    #             points[:, 0] = 1.0 - points[:, 0]
    #             mirrored_landmarks['hands'][hand_type] = points
    #         else:
    #             mirrored_landmarks['hands'][hand_type] = landmarks_3d['hands'][hand_type]
    #
    #     # Mirror face landmarks
    #     if len(landmarks_3d['face']) > 0:
    #         points = landmarks_3d['face'].copy()
    #         # Mirror X coordinates (flip horizontally)
    #         points[:, 0] = 1.0 - points[:, 0]
    #         mirrored_landmarks['face'] = points
    #     else:
    #         mirrored_landmarks['face'] = landmarks_3d['face']
    #
    #     # Mirror pose landmarks
    #     if len(landmarks_3d['pose']) > 0:
    #         points = landmarks_3d['pose'].copy()
    #         # Mirror X coordinates (flip horizontally)
    #         points[:, 0] = 1.0 - points[:, 0]
    #         mirrored_landmarks['pose'] = points
    #         # Copy pose names if they exist
    #         if 'pose_names' in landmarks_3d:
    #             mirrored_landmarks['pose_names'] = landmarks_3d['pose_names']
    #     else:
    #         mirrored_landmarks['pose'] = landmarks_3d['pose']
    #         if 'pose_names' in landmarks_3d:
    #             mirrored_landmarks['pose_names'] = landmarks_3d.get('pose_names', [])
    #
    #     return mirrored_landmarks

    # def _recalibrate_after_mirroring(self, mirrored_landmarks: Dict, original_pose_wrists: Dict) -> Dict:
    #     """
    #     Recalibrate hand positions after mirroring to use the correct wrist mapping
    #
    #     After mirroring:
    #     - left_hand landmarks are on the right side → should align with RIGHT_WRIST
    #     - right_hand landmarks are on the left side → should align with LEFT_WRIST
    #     """
    #     # Check if we have any pose wrists to work with
    #     if (original_pose_wrists.get("LEFT_WRIST") is None and
    #             original_pose_wrists.get("RIGHT_WRIST") is None):
    #         print("DEBUG: No pose wrists available for recalibration")
    #         return mirrored_landmarks  # No calibration possible
    #
    #     # Mirror the pose wrists too (they follow the same mirroring as everything else)
    #     mirrored_pose_wrists = {}
    #
    #     if original_pose_wrists.get("LEFT_WRIST") is not None:
    #         mirrored_left_wrist = original_pose_wrists["LEFT_WRIST"].copy()
    #         mirrored_left_wrist[0] = 1.0 - mirrored_left_wrist[0]  # Mirror X coordinate
    #         mirrored_pose_wrists["LEFT_WRIST"] = mirrored_left_wrist
    #
    #     if original_pose_wrists.get("RIGHT_WRIST") is not None:
    #         mirrored_right_wrist = original_pose_wrists["RIGHT_WRIST"].copy()
    #         mirrored_right_wrist[0] = 1.0 - mirrored_right_wrist[0]  # Mirror X coordinate
    #         mirrored_pose_wrists["RIGHT_WRIST"] = mirrored_right_wrist
    #
    #     # Now recalibrate with swapped mapping
    #     corrected_hand_mapping = {
    #         'left_hand': 'RIGHT_WRIST',  # After mirroring, left_hand should align with RIGHT_WRIST
    #         'right_hand': 'LEFT_WRIST'  # After mirroring, right_hand should align with LEFT_WRIST
    #     }
    #
    #     for hand_type in ['left_hand', 'right_hand']:
    #         hand_landmarks = mirrored_landmarks['hands'].get(hand_type, [])
    #
    #         if len(hand_landmarks) > 0:
    #             points = hand_landmarks
    #             wrist_name = corrected_hand_mapping[hand_type]
    #             target_wrist_position = mirrored_pose_wrists.get(wrist_name)
    #
    #             if target_wrist_position is not None:
    #                 # Get current hand wrist position (landmark 0)
    #                 current_hand_wrist = points[0]
    #
    #                 # Calculate new translation offset
    #                 translation_offset = target_wrist_position - current_hand_wrist
    #
    #                 # Apply translation to all hand landmarks
    #                 mirrored_landmarks['hands'][hand_type] = points + translation_offset
    #                 print(f"DEBUG: Recalibrated {hand_type} to {wrist_name}")
    #             else:
    #                 print(f"DEBUG: No target wrist {wrist_name} for {hand_type}")
    #
    #     mirrored_pose_nose = None
    #     if original_pose_wrists and hasattr(self, '_original_pose_nose'):
    #         mirrored_pose_nose = self._original_pose_nose.copy()
    #         mirrored_pose_nose[0] = 1.0 - mirrored_pose_nose[0]  # Mirror X coordinate
    #
    #     # Recalibrate face if we have both face landmarks and mirrored pose nose
    #     if (mirrored_pose_nose is not None and
    #             len(mirrored_landmarks['face']) > 0):
    #         face_points = mirrored_landmarks['face']
    #         # Use first point as face nose reference
    #         current_face_nose = face_points[0]
    #
    #         # Calculate new translation offset
    #         translation_offset = mirrored_pose_nose - current_face_nose
    #
    #         # Apply translation to all face landmarks
    #         mirrored_landmarks['face'] = face_points + translation_offset
    #         print(f"DEBUG: Recalibrated face to mirrored pose nose")
    #
    #         # Mirror the pose nose position
    #         mirrored_pose_nose = None
    #         if original_pose_wrists and hasattr(self, '_original_pose_nose'):
    #             mirrored_pose_nose = self._original_pose_nose.copy()
    #             mirrored_pose_nose[0] = 1.0 - mirrored_pose_nose[0]  # Mirror X coordinate
    #
    #         # Recalibrate face if we have both face landmarks and mirrored pose nose
    #         if (mirrored_pose_nose is not None and
    #                 len(mirrored_landmarks['face']) > 0):
    #             face_points = mirrored_landmarks['face']
    #             # Use first point as face nose reference
    #             current_face_nose = face_points[0]
    #
    #             # Calculate new translation offset
    #             translation_offset = mirrored_pose_nose - current_face_nose
    #
    #             # Apply translation to all face landmarks
    #             mirrored_landmarks['face'] = face_points + translation_offset
    #             print(f"DEBUG: Recalibrated face to mirrored pose nose")
    #
    #     return mirrored_landmarks


    # def _extract_landmarks_for_frame_corrected(self, frame_key: str) -> Dict:
    #     """Extract 3D coordinates for a specific frame using corrected hand data WITH CORRECTED CALIBRATION AFTER MIRRORING"""
    #     print("DEBUG: using the corrected extract_landmarks_for_frame WITH CORRECTED CALIBRATION")
    #     if self.hand_tracker and hasattr(self.hand_tracker, 'corrected_frames_data'):
    #         # Use corrected frame data if available
    #         frame_data = self.hand_tracker.get_corrected_frame_data(frame_key)
    #     else:
    #         # Fall back to original data
    #         frame_data = self.frames_data.get(frame_key, {})
    #
    #     landmarks_3d = {
    #         'hands': {'left_hand': [], 'right_hand': []},
    #         'face': [],
    #         'pose': []
    #     }
    #
    #     # First, extract pose landmarks to get wrist positions for calibration
    #     pose_wrists = {"LEFT_WRIST": None, "RIGHT_WRIST": None}
    #     pose_data = frame_data.get('pose', {})
    #
    #     if pose_data and "NOSE" in pose_data:
    #         self._original_pose_nose = np.array([
    #             pose_data["NOSE"]["x"],
    #             pose_data["NOSE"]["y"],
    #             pose_data["NOSE"]["z"]
    #         ])
    #
    #     if pose_data:
    #         # Use original pose data for calibration
    #         if "LEFT_WRIST" in pose_data:
    #             pose_wrists["LEFT_WRIST"] = np.array([
    #                 pose_data["LEFT_WRIST"]["x"],
    #                 pose_data["LEFT_WRIST"]["y"],
    #                 pose_data["LEFT_WRIST"]["z"]
    #             ])
    #
    #         if "RIGHT_WRIST" in pose_data:
    #             pose_wrists["RIGHT_WRIST"] = np.array([
    #                 pose_data["RIGHT_WRIST"]["x"],
    #                 pose_data["RIGHT_WRIST"]["y"],
    #                 pose_data["RIGHT_WRIST"]["z"]
    #             ])
    #
    #     # Extract and calibrate hand landmarks
    #     hands_data = frame_data.get('hands', {})
    #     hand_to_wrist_mapping = {
    #         'left_hand': 'LEFT_WRIST',
    #         'right_hand': 'RIGHT_WRIST'
    #     }
    #
    #     for hand_type in ['left_hand', 'right_hand']:
    #         hand_info = hands_data.get(hand_type, {})
    #         wrist_name = hand_to_wrist_mapping[hand_type]
    #         pose_wrist_position = pose_wrists.get(wrist_name)
    #
    #         if isinstance(hand_info, dict) and 'landmarks' in hand_info:
    #             hand_landmarks = hand_info['landmarks']
    #         elif isinstance(hand_info, list):
    #             hand_landmarks = hand_info
    #         else:
    #             hand_landmarks = []
    #
    #         if hand_landmarks:
    #             # Convert to numpy array for easier manipulation
    #             points = np.array([[lm['x'], lm['y'], lm['z']] for lm in hand_landmarks])
    #             points[:, 2] *= HANDS_AMPLIFICATION  # amplify to accommodate small z-range
    #
    #             # Calibrate hand position to pose wrist if both are available
    #             if pose_wrist_position is not None and len(points) > 0:
    #                 # Hand wrist is landmark 0
    #                 hand_wrist_position = points[0]
    #
    #                 # Calculate translation offset
    #                 translation_offset = pose_wrist_position - hand_wrist_position
    #
    #                 # Apply translation to all hand landmarks
    #                 points = points + translation_offset
    #
    #             landmarks_3d['hands'][hand_type] = points
    #
    #     # Extract pose landmarks
    #     if pose_data:
    #         mediapipe_pose_landmarks = [
    #             'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER',
    #             'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT',
    #             'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',
    #             'LEFT_WRIST', 'RIGHT_WRIST',
    #             # 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX',  'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB',
    #             'LEFT_HIP', 'RIGHT_HIP',
    #             'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL',
    #             'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'
    #         ]
    #
    #         pose_points = []
    #         pose_landmark_names = []
    #
    #         for landmark_name in mediapipe_pose_landmarks:
    #             if landmark_name in pose_data:
    #                 lm = pose_data[landmark_name]
    #                 visibility = lm.get('visibility', 1.0)
    #                 if visibility > 0.5:
    #                     pose_points.append([lm['x'], lm['y'], lm['z']])
    #                     pose_landmark_names.append(landmark_name)
    #
    #         if pose_points:
    #             landmarks_3d['pose'] = np.array(pose_points)
    #             landmarks_3d['pose_names'] = pose_landmark_names
    #
    #     # Extract face landmarks
    #     face_data = frame_data.get('face', {})
    #     if 'all_landmarks' in face_data and face_data['all_landmarks']:
    #         face_landmarks = face_data['all_landmarks']
    #         sampled_landmarks = face_landmarks[::2]
    #         points = np.array([[lm['x'], lm['y'], lm['z']] for lm in sampled_landmarks])
    #         points[:, 2] *= FACE_AMPLIFICATION
    #
    #         # ADD MISSING FACE CALIBRATION
    #         if pose_data and "NOSE" in pose_data and len(sampled_landmarks) > 1:
    #             pose_nose_position = np.array([
    #                 pose_data["NOSE"]["x"],
    #                 pose_data["NOSE"]["y"],
    #                 pose_data["NOSE"]["z"]
    #             ])
    #
    #             # Use first sampled landmark as nose reference
    #             face_nose_position = np.array([
    #                 sampled_landmarks[0]['x'],
    #                 sampled_landmarks[0]['y'],
    #                 sampled_landmarks[0]['z']
    #             ])
    #             face_nose_position[2] *= FACE_AMPLIFICATION
    #
    #             # Calculate and apply translation offset
    #             translation_offset = pose_nose_position - face_nose_position
    #             points = points + translation_offset
    #
    #         landmarks_3d['face'] = points
    #
    #     # Apply mirroring to all landmarks
    #     mirrored_landmarks = self._apply_mirroring_to_landmarks(landmarks_3d)
    #
    #     # CRITICAL FIX: After mirroring, recalibrate with corrected wrist mapping
    #     mirrored_landmarks = self._recalibrate_after_mirroring(mirrored_landmarks, pose_wrists)
    #
    #     return mirrored_landmarks

    def _extract_landmarks_for_frame(self, frame_key: str) -> Dict:
        """
        Extract landmarks from processed frame data (already calibrated and mirrored)

        Args:
            frame_key: Frame identifier

        Returns:
            Dictionary with extracted landmarks ready for 3D visualization
        """
        frame_data = self.frames_data.get(frame_key, {})

        landmarks_3d = {
            'hands': {'left_hand': [], 'right_hand': []},
            'face': [],
            'pose': []
        }

        # Extract hands - data is already processed and calibrated
        hands_data = frame_data.get('hands', {})
        for hand_type in ['left_hand', 'right_hand']:
            hand_info = hands_data.get(hand_type, [])

            # Handle both old and new JSON formats
            if isinstance(hand_info, dict) and 'landmarks' in hand_info:
                landmarks = hand_info['landmarks']
            elif isinstance(hand_info, list):
                landmarks = hand_info
            else:
                landmarks = []

            if landmarks:
                points = np.array([[lm['x'], lm['y'], lm['z']] for lm in landmarks])
                landmarks_3d['hands'][hand_type] = points

        # Extract face - data is already processed and calibrated
        face_data = frame_data.get('face', {})
        if 'all_landmarks' in face_data and face_data['all_landmarks']:
            face_landmarks = face_data['all_landmarks']
            sampled_landmarks = face_landmarks[::2]  # Sample every 2nd landmark for performance
            points = np.array([[lm['x'], lm['y'], lm['z']] for lm in sampled_landmarks])
            landmarks_3d['face'] = points

        # Extract pose - data is already processed
        pose_data = frame_data.get('pose', {})
        if pose_data:
            pose_points = []
            pose_names = []

            for landmark_name, lm_data in pose_data.items():
                pose_points.append([lm_data['x'], lm_data['y'], lm_data['z']])
                pose_names.append(landmark_name)

            if pose_points:
                landmarks_3d['pose'] = np.array(pose_points)
                landmarks_3d['pose_names'] = pose_names

        # Apply anchored amplification for better depth visualization
        landmarks_3d = self._apply_anchored_amplification(landmarks_3d)

        return landmarks_3d

    def precompute_paths_from_json(self, frames_data: dict):
        """Precompute all hand paths from JSON data WITHOUT hand swapping"""
        print("Precomputing hand paths without hand swapping...")

        # Sort frame keys numerically
        sorted_frame_keys = sorted(frames_data.keys(), key=int)

        # First pass: collect all hand data (no consistency correction needed)
        corrected_frames_data = {}

        print(f"Processing {len(sorted_frame_keys)} frames...")

        for i, frame_key in enumerate(sorted_frame_keys):
            frame_data = frames_data[frame_key].copy()
            # No hand correction - use original data as-is
            corrected_frames_data[frame_key] = frame_data

            # Progress indicator
            if (i + 1) % 50 == 0 or i == len(sorted_frame_keys) - 1:
                print(f"  Processed {i + 1}/{len(sorted_frame_keys)} frames...")

        print("Building hand movement paths from original data...")

        # # Second pass: build paths from original data with hand-wrist calibration
        # for frame_key in sorted_frame_keys:
        #     frame_data = corrected_frames_data[frame_key]
        #     hands_data = frame_data.get('hands', {})
        #     pose_data = frame_data.get('pose', {})
        #
        #     # Extract pose wrist positions for calibration
        #     pose_wrists = {"LEFT_WRIST": None, "RIGHT_WRIST": None}
        #     if pose_data:
        #         if "LEFT_WRIST" in pose_data:
        #             pose_wrists["LEFT_WRIST"] = np.array([
        #                 pose_data["LEFT_WRIST"]["x"],
        #                 pose_data["LEFT_WRIST"]["y"],
        #                 pose_data["LEFT_WRIST"]["z"]
        #             ])
        #
        #         if "RIGHT_WRIST" in pose_data:
        #             pose_wrists["RIGHT_WRIST"] = np.array([
        #                 pose_data["RIGHT_WRIST"]["x"],
        #                 pose_data["RIGHT_WRIST"]["y"],
        #                 pose_data["RIGHT_WRIST"]["z"]
        #             ])
        #
        #     # NO HAND SWAPPING - use original hand mapping
        #     hand_to_wrist_mapping = {
        #         'left_hand': 'LEFT_WRIST',
        #         'right_hand': 'RIGHT_WRIST'
        #     }
        #
        #     # Process both hands with calibration (no swapping)
        #     for hand_type in ['left_hand', 'right_hand']:
        #         hand_data = hands_data.get(hand_type, [])  # Use original hand data
        #         wrist_name = hand_to_wrist_mapping[hand_type]
        #         pose_wrist_position = pose_wrists.get(wrist_name)
        #
        #         if hand_data:
        #             # Handle both old and new JSON formats
        #             if isinstance(hand_data, dict) and 'landmarks' in hand_data:
        #                 landmarks = hand_data['landmarks']
        #             else:
        #                 landmarks = hand_data
        #
        #             if landmarks and len(landmarks) > self.wrist_index:
        #                 wrist_point = landmarks[self.wrist_index]
        #                 hand_wrist_position = np.array([wrist_point['x'], wrist_point['y'], wrist_point['z']])
        #
        #                 # Apply calibration if pose wrist is available
        #                 if pose_wrist_position is not None:
        #                     # Use pose wrist position instead of detected hand wrist
        #                     calibrated_position = pose_wrist_position
        #                 else:
        #                     # Fall back to detected hand wrist position
        #                     calibrated_position = hand_wrist_position
        #
        #                 # Store the path point
        #                 path_data = {
        #                     'frame': int(frame_key),
        #                     'point': calibrated_position.tolist(),
        #                     'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0),
        #                     'calibrated': pose_wrist_position is not None
        #                 }
        #
        #                 if hand_type == 'left_hand':
        #                     self.full_left_hand_path.append(path_data)
        #                 else:
        #                     self.full_right_hand_path.append(path_data)

        # Store corrected data for use in visualization
        self.corrected_frames_data = corrected_frames_data

        # Print statistics (no corrections since we're not swapping)
        print(f"\nHand Path Results (No Swapping Applied):")
        print(f"  Original data used as-is")
        print(f"  Left hand: {len(self.full_left_hand_path)} points")
        print(f"  Right hand: {len(self.full_right_hand_path)} points")

        # Count calibrated frames
        calibrated_left = sum(1 for p in self.full_left_hand_path if p.get('calibrated', False))
        calibrated_right = sum(1 for p in self.full_right_hand_path if p.get('calibrated', False))

        print(f"  Calibrated frames: Left {calibrated_left}, Right {calibrated_right}")

        # Validate path consistency
        self._validate_path_consistency()

    def _draw_pose_connections(self, points: np.ndarray, pose_names: list = None):
        """Draw connections between pose landmarks using MediaPipe's pose structure"""
        if len(points) == 0:
            return

        # Define MediaPipe pose connections (index pairs)
        # These are the standard connections used by MediaPipe
        mediapipe_pose_connections = [
            # Face connections
            # ('NOSE', 'LEFT_EYE_INNER'), ('LEFT_EYE_INNER', 'LEFT_EYE'),
            # ('LEFT_EYE', 'LEFT_EYE_OUTER'), ('LEFT_EYE_OUTER', 'LEFT_EAR'),
            # ('NOSE', 'RIGHT_EYE_INNER'), ('RIGHT_EYE_INNER', 'RIGHT_EYE'),
            # ('RIGHT_EYE', 'RIGHT_EYE_OUTER'), ('RIGHT_EYE_OUTER', 'RIGHT_EAR'),
            # ('MOUTH_LEFT', 'MOUTH_RIGHT'),

            # Torso connections
            ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
            ('LEFT_SHOULDER', 'LEFT_HIP'), ('RIGHT_SHOULDER', 'RIGHT_HIP'),
            ('LEFT_HIP', 'RIGHT_HIP'),

            # Left arm connections
            ('LEFT_SHOULDER', 'LEFT_ELBOW'), ('LEFT_ELBOW', 'LEFT_WRIST'),
            # ('LEFT_WRIST', 'LEFT_PINKY'), ('LEFT_WRIST', 'LEFT_INDEX'),
            # ('LEFT_WRIST', 'LEFT_THUMB'), ('LEFT_PINKY', 'LEFT_INDEX'),

            # Right arm connections
            ('RIGHT_SHOULDER', 'RIGHT_ELBOW'), ('RIGHT_ELBOW', 'RIGHT_WRIST'),
            # ('RIGHT_WRIST', 'RIGHT_PINKY'), ('RIGHT_WRIST', 'RIGHT_INDEX'),
            # ('RIGHT_WRIST', 'RIGHT_THUMB'), ('RIGHT_PINKY', 'RIGHT_INDEX'),

            # Left leg connections
            ('LEFT_HIP', 'LEFT_KNEE'), ('LEFT_KNEE', 'LEFT_ANKLE'),
            ('LEFT_ANKLE', 'LEFT_HEEL'), ('LEFT_ANKLE', 'LEFT_FOOT_INDEX'),
            ('LEFT_HEEL', 'LEFT_FOOT_INDEX'),

            # Right leg connections
            ('RIGHT_HIP', 'RIGHT_KNEE'), ('RIGHT_KNEE', 'RIGHT_ANKLE'),
            ('RIGHT_ANKLE', 'RIGHT_HEEL'), ('RIGHT_ANKLE', 'RIGHT_FOOT_INDEX'),
            ('RIGHT_HEEL', 'RIGHT_FOOT_INDEX'),
        ]

        # If we don't have pose names, fall back to simplified connections
        if pose_names is None or len(pose_names) != len(points):
            # Simplified fallback: connect major body parts if we have enough points
            if len(points) >= 4:
                # Try to connect what we assume are major landmarks
                color = self.colors['pose']
                # Connect first few points (assuming they're major landmarks)
                for i in range(min(4, len(points) - 1)):
                    self.ax.plot([points[i][0], points[i + 1][0]],
                                 [points[i][1], points[i + 1][1]],
                                 [points[i][2], points[i + 1][2]],
                                 color=color, alpha=0.6, linewidth=2)
            return

        # Create a mapping from landmark names to indices
        name_to_index = {name: i for i, name in enumerate(pose_names)}

        # Draw connections based on MediaPipe's structure
        color = self.colors['pose']
        connections_drawn = 0

        for start_name, end_name in mediapipe_pose_connections:
            if start_name in name_to_index and end_name in name_to_index:
                start_idx = name_to_index[start_name]
                end_idx = name_to_index[end_name]

                start_point = points[start_idx]
                end_point = points[end_idx]

                # Draw the connection
                self.ax.plot([start_point[0], end_point[0]],
                             [start_point[1], end_point[1]],
                             [start_point[2], end_point[2]],
                             color=color, alpha=0.7, linewidth=2)
                connections_drawn += 1

        # If no connections were drawn, fall back to simplified approach
        if connections_drawn == 0 and len(points) >= 2:
            print(f"Warning: No pose connections found, using simplified visualization")
            # Connect shoulders if we can identify them
            shoulder_connections = [
                ('LEFT_SHOULDER', 'RIGHT_SHOULDER'),
                ('LEFT_SHOULDER', 'LEFT_ELBOW'),
                ('RIGHT_SHOULDER', 'RIGHT_ELBOW')
            ]

            for start_name, end_name in shoulder_connections:
                if start_name in name_to_index and end_name in name_to_index:
                    start_idx = name_to_index[start_name]
                    end_idx = name_to_index[end_name]

                    start_point = points[start_idx]
                    end_point = points[end_idx]

                    self.ax.plot([start_point[0], end_point[0]],
                                 [start_point[1], end_point[1]],
                                 [start_point[2], end_point[2]],
                                 color=color, alpha=0.7, linewidth=2)

    def _draw_frame(self, frame_num: int):
        """Draw landmarks for a specific frame"""
        self.ax.clear()

        # Set up plot again
        self.ax.set_xlabel('X')
        self.ax.set_ylabel('Y')
        self.ax.set_zlabel('Z')
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 1)
        self.ax.set_zlim(-2, 2)
        self.ax.invert_yaxis()

        # Get frame key
        frame_keys = sorted(self.frames_data.keys(), key=int)
        if frame_num >= len(frame_keys):
            frame_num = len(frame_keys) - 1

        frame_key = frame_keys[frame_num]
        current_frame_number = int(frame_key)

        # Extract landmarks (already processed - no calibration needed)
        landmarks = self._extract_landmarks_for_frame(frame_key)

        # Draw hand landmarks
        for hand_type, points in landmarks['hands'].items():
            if len(points) > 0:
                color = self.colors['hands'][hand_type]
                self.ax.scatter(points[:, 0], points[:, 1], points[:, 2],
                                c=color, s=30, alpha=0.8, label=f'{hand_type.replace("_", " ").title()}')

                # Draw hand connections
                self._draw_hand_connections(points, color)

        # Draw face landmarks
        if len(landmarks['face']) > 0:
            self.ax.scatter(landmarks['face'][:, 0], landmarks['face'][:, 1], landmarks['face'][:, 2],
                            c=self.colors['face'], s=10, alpha=0.6, label='Face')

        # Draw pose landmarks with connections
        if len(landmarks['pose']) > 0:
            self.ax.scatter(landmarks['pose'][:, 0], landmarks['pose'][:, 1], landmarks['pose'][:, 2],
                            c=self.colors['pose'], s=50, alpha=0.8, label='Pose')

            # Draw pose connections
            pose_names = landmarks.get('pose_names', None)
            self._draw_pose_connections(landmarks['pose'], pose_names)

        # Draw hand paths if tracking is enabled
        if self.hand_tracker:
            show_full_path = not hasattr(self, '_is_animating') or not self._is_animating
            self.hand_tracker.draw_paths_3d(self.ax, current_frame_number, show_full_path)

        # Update title with frame info
        frame_timestamp = float(frame_key) / self.metadata.get('fps', 30)
        tracking_status = " (Hand Tracking)" if self.track_hands else ""
        self.ax.set_title(f'Frame {frame_key} (t={frame_timestamp:.2f}s){tracking_status}\n'
                          f'Source: {self.metadata.get("input_source", "Unknown")}')

        # Add legend
        self.ax.legend(loc='upper left', bbox_to_anchor=(0, 1))

        return self.ax

    def _apply_anchored_amplification(self, landmarks_3d: Dict) -> Dict:
        """
        Apply Z-amplification to hands and face while keeping them anchored to pose reference points

        Args:
            landmarks_3d: Dictionary with hands, face, and pose landmarks

        Returns:
            Dictionary with amplified but anchored landmarks
        """
        amplified_landmarks = landmarks_3d.copy()

        # Get pose reference points (anchors)
        pose_points = landmarks_3d.get('pose', [])
        pose_names = landmarks_3d.get('pose_names', [])

        if len(pose_points) == 0 or len(pose_names) == 0:
            return landmarks_3d  # No pose data, return as-is

        # Create pose name-to-index mapping
        pose_name_to_idx = {name: i for i, name in enumerate(pose_names)}

        # Amplify hands relative to their corresponding wrist anchors
        for hand_type in ['left_hand', 'right_hand']:
            hand_points = landmarks_3d['hands'].get(hand_type, [])

            if len(hand_points) > 0:
                # Determine which wrist to use as anchor
                wrist_name = 'LEFT_WRIST' if hand_type == 'left_hand' else 'RIGHT_WRIST'

                if wrist_name in pose_name_to_idx:
                    wrist_idx = pose_name_to_idx[wrist_name]
                    anchor_point = pose_points[wrist_idx]  # This is the anchor (pose wrist)

                    # Calculate relative positions from anchor
                    relative_positions = hand_points - anchor_point

                    # Amplify only the Z-component of relative positions
                    relative_positions[:, 2] *= HANDS_AMPLIFICATION

                    # Add back to anchor to get final positions
                    amplified_hand_points = anchor_point + relative_positions

                    amplified_landmarks['hands'][hand_type] = amplified_hand_points


        # Amplify face relative to pose nose anchor
        face_points = landmarks_3d.get('face', [])
        if len(face_points) > 0 and 'NOSE' in pose_name_to_idx:
            nose_idx = pose_name_to_idx['NOSE']
            anchor_point = pose_points[nose_idx]  # This is the anchor (pose nose)

            # Calculate relative positions from anchor
            relative_positions = face_points - anchor_point

            # Amplify only the Z-component of relative positions
            relative_positions[:, 2] *= FACE_AMPLIFICATION

            # Add back to anchor to get final positions
            amplified_face_points = anchor_point + relative_positions

            amplified_landmarks['face'] = amplified_face_points

        return amplified_landmarks



    def visualize_static(self, frame_number: Optional[int] = None):
        """Display a static 3D visualization of landmarks for a specific frame"""
        self._setup_3d_plot()

        # Mark as not animating for full path display
        self._is_animating = False

        if frame_number is None:
            frame_number = 0

        self._draw_frame(frame_number)

        # Add instructions
        instruction_text = 'Use mouse to rotate view. Close window to exit.'
        if self.track_hands:
            instruction_text += '\nShowing complete hand paths.'

        plt.figtext(0.02, 0.95, instruction_text, fontsize=10, style='italic')

        plt.tight_layout()
        plt.show()

    def visualize_animated(self, interval: float = 100):
        """Display an animated 3D visualization of landmarks across all frames"""
        # Set up the 3D plot FIRST - this creates self.fig and self.ax
        self._setup_3d_plot()

        # Set animation flag for progressive path tracking
        self._is_animating = True

        frame_keys = sorted(self.frames_data.keys(), key=int)
        total_frames = len(frame_keys)

        def animate(frame):
            return self._draw_frame(frame)

        # Create animation - now self.fig exists
        self.anim = FuncAnimation(self.fig, animate, frames=total_frames,
                                  interval=interval, blit=False, repeat=True)

        # Add instructions
        instruction_text = 'Animation playing. Use mouse to rotate view. Close window to exit.'

        plt.figtext(0.02, 0.02, instruction_text, fontsize=10, style='italic')

        plt.tight_layout()
        plt.show()

        return self.anim

    def save_animation(self, output_path: str, fps: float = 10):
        """Save the 3D animation as a video file"""
        self._setup_3d_plot()

        frame_keys = sorted(self.frames_data.keys(), key=int)
        total_frames = len(frame_keys)

        def animate(frame):
            return self._draw_frame(frame)

        # Create animation
        anim = FuncAnimation(self.fig, animate, frames=total_frames,
                             interval=1000 / fps, blit=False, repeat=False)

        # Save animation
        print(f"Saving 3D animation to {output_path}...")
        anim.save(output_path, writer='pillow', fps=fps)
        print(f"Animation saved successfully!")

        plt.close()

    def analyze_hand_paths(self):
        """Analyze and print detailed hand path information"""
        if not self.hand_tracker:
            print("Hand tracking is not enabled. Use track_hands=True to enable analysis.")
            return

        stats = self.hand_tracker.get_path_statistics()

        print("\n" + "=" * 60)
        print("HAND PATH ANALYSIS")
        print("=" * 60)

        print(f"\nVideo Information:")
        print(f"  Source: {self.metadata.get('input_source', 'Unknown')}")
        print(f"  FPS: {self.metadata.get('fps', 'Unknown')}")
        print(f"  Total frames: {len(self.frames_data)}")

        print(f"\nLeft Hand Movement:")
        print(f"  Detected in {stats['left_hand']['total_points']} frames")
        if stats['left_hand']['total_points'] > 0:
            print(f"  Total distance traveled: {stats['left_hand']['total_distance']:.3f} units")
            # print(f"  Average speed: {stats['left_hand']['avg_speed']:.3f} units/second")
            # print(f"  Maximum speed: {stats['left_hand']['max_speed']:.3f} units/second")

        print(f"\nRight Hand Movement:")
        print(f"  Detected in {stats['right_hand']['total_points']} frames")
        if stats['right_hand']['total_points'] > 0:
            print(f"  Total distance traveled: {stats['right_hand']['total_distance']:.3f} units")
            # print(f"  Average speed: {stats['right_hand']['avg_speed']:.3f} units/second")
            # print(f"  Maximum speed: {stats['right_hand']['max_speed']:.3f} units/second")

        # Detect periods of high activity
        print(f"\nMovement Analysis:")
        if self.hand_tracker.full_left_hand_path or self.hand_tracker.full_right_hand_path:
            print("  Use the 3D visualization to see detailed movement patterns.")
            print("  Green markers = Start positions")
            print("  Red/Blue markers = Current/End positions")
            print("  Path opacity increases toward the end of movement")

        print("=" * 60)


class HandPathTracker:
    """Tracks and visualizes hand movement paths from JSON data with advanced consistency correction"""

    def __init__(self, max_path_length: int = 50):
        """
        Initialize hand path tracker

        Args:
            max_path_length: Maximum number of points to keep in path history
        """
        self.max_path_length = max_path_length
        self.left_hand_path = deque(maxlen=max_path_length)
        self.right_hand_path = deque(maxlen=max_path_length)

        # Store full paths for analysis
        self.full_left_hand_path = []
        self.full_right_hand_path = []

        # Path colors (slightly transparent)
        self.path_colors = {
            'left_hand': (1.0, 0.5, 0.5, 0.7),  # Light red with alpha
            'right_hand': (0.5, 0.5, 1.0, 0.7)  # Light blue with alpha
        }

        # Wrist landmark index (MediaPipe hand landmark 0 is wrist)
        self.wrist_index = 0

    def precompute_paths_from_json(self, frames_data: dict):
        """Precompute all hand paths from JSON data with advanced consistency correction"""
        print("Precomputing hand paths with advanced consistency correction...")

        # Sort frame keys numerically
        sorted_frame_keys = sorted(frames_data.keys(), key=int)

        # First pass: collect all hand data and apply consistency correction
        corrected_frames_data = {}

        print(f"Processing {len(sorted_frame_keys)} frames for hand consistency...")

        for i, frame_key in enumerate(sorted_frame_keys):
            frame_data = frames_data[frame_key].copy()
            hands_data = frame_data.get('hands', {})

            # Progress indicator
            if (i + 1) % 50 == 0 or i == len(sorted_frame_keys) - 1:
                print(f"  Processed {i + 1}/{len(sorted_frame_keys)} frames...")

        print("Building hand movement paths from corrected data...")

        # Second pass: build paths from corrected data
        for frame_key in sorted_frame_keys:
            frame_data = frames_data[frame_key]
            hands_data = frame_data.get('hands', {})

            # Process left hand
            left_hand = hands_data.get('left_hand', [])
            if left_hand:
                # Handle both old and new JSON formats
                if isinstance(left_hand, dict) and 'landmarks' in left_hand:
                    landmarks = left_hand['landmarks']
                else:
                    landmarks = left_hand

                if landmarks and len(landmarks) > self.wrist_index:
                    wrist_point = landmarks[self.wrist_index]
                    point = [wrist_point['x'], wrist_point['y'], wrist_point['z']]
                    self.full_left_hand_path.append({
                        'frame': int(frame_key),
                        'point': point,
                        'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0)
                    })

            # Process right hand
            right_hand = hands_data.get('right_hand', [])
            if right_hand:
                # Handle both old and new JSON formats
                if isinstance(right_hand, dict) and 'landmarks' in right_hand:
                    landmarks = right_hand['landmarks']
                else:
                    landmarks = right_hand

                if landmarks and len(landmarks) > self.wrist_index:
                    wrist_point = landmarks[self.wrist_index]
                    point = [wrist_point['x'], wrist_point['y'], wrist_point['z']]
                    self.full_right_hand_path.append({
                        'frame': int(frame_key),
                        'point': point,
                        'timestamp': frame_data.get('timestamp', int(frame_key) / 30.0)
                    })

        # Validate path consistency
        self._validate_path_consistency()

    def _validate_path_consistency(self):
        """Validate that the paths look consistent (no major jumps)"""
        print("\nValidating path consistency...")

        def check_path_jumps(path_data, hand_name):
            if len(path_data) < 2:
                return 0

            jump_count = 0
            jump_threshold = 0.3  # Large movement threshold

            for i in range(1, len(path_data)):
                p1 = np.array(path_data[i - 1]['point'])
                p2 = np.array(path_data[i]['point'])
                distance = np.linalg.norm(p2 - p1)

                if distance > jump_threshold:
                    jump_count += 1

            return jump_count

        left_jumps = check_path_jumps(self.full_left_hand_path, "Left")
        right_jumps = check_path_jumps(self.full_right_hand_path, "Right")

        print(f"  Left hand large movements: {left_jumps}")
        print(f"  Right hand large movements: {right_jumps}")

        if left_jumps + right_jumps < 5:
            print("Paths look consistent!")
        else:
            print("Some large movements detected - this might be normal or indicate remaining issues")

    def get_corrected_frame_data(self, frame_key: str):
        """Get corrected frame data for a specific frame"""
        return self.corrected_frames_data.get(frame_key, {})

    def update_paths_for_frame(self, target_frame: int):
        """Update visible paths up to the target frame"""
        self.left_hand_path.clear()
        self.right_hand_path.clear()

        # Add left hand points up to target frame
        for path_point in self.full_left_hand_path:
            if path_point['frame'] <= target_frame:
                self.left_hand_path.append(path_point['point'])
            else:
                break

        # Add right hand points up to target frame
        for path_point in self.full_right_hand_path:
            if path_point['frame'] <= target_frame:
                self.right_hand_path.append(path_point['point'])
            else:
                break

    def get_path_for_frame_range(self, start_frame: int, end_frame: int, hand_type: str):
        """Get path points for a specific frame range"""
        if hand_type == 'left_hand':
            path_data = self.full_left_hand_path
        elif hand_type == 'right_hand':
            path_data = self.full_right_hand_path
        else:
            return []

        return [point['point'] for point in path_data
                if start_frame <= point['frame'] <= end_frame]

    def draw_paths_3d(self, ax, current_frame: int = None, show_full_path: bool = False):
        """Draw hand paths on 3D plot using calibrated 3D positions"""

        # Get the parent visualizer instance to access landmark extraction methods
        visualizer = None
        # Find the visualizer instance that owns this tracker
        import inspect
        for frame_info in inspect.stack():
            frame_locals = frame_info.frame.f_locals
            if 'self' in frame_locals and hasattr(frame_locals['self'], '_extract_landmarks_for_frame'):
                visualizer = frame_locals['self']
                break

        if visualizer is None:
            print("Warning: Could not find visualizer instance for 3D calibration")
            return

        # Build 3D calibrated paths on-the-fly
        left_points_3d = []
        right_points_3d = []

        # Sort frame keys numerically
        sorted_frame_keys = sorted(visualizer.frames_data.keys(), key=int)

        for frame_key in sorted_frame_keys:
            frame_num = int(frame_key)
            if not show_full_path and current_frame is not None and frame_num > current_frame:
                break

            # Extract calibrated 3D landmarks for this frame
            # if hasattr(self, 'corrected_frames_data') and self.corrected_frames_data:
            #     # Use corrected method if we have corrected data
            #     landmarks_3d = visualizer._extract_landmarks_for_frame_corrected(frame_key)
            # else:
            #     # Use regular method
            #     landmarks_3d = visualizer._extract_landmarks_for_frame(frame_key)

            landmarks_3d = visualizer._extract_landmarks_for_frame(frame_key)

            # Extract wrist positions from calibrated 3D landmarks
            left_hand_landmarks = landmarks_3d['hands']['left_hand']
            right_hand_landmarks = landmarks_3d['hands']['right_hand']

            # Add left hand wrist (landmark 0) if available
            if len(left_hand_landmarks) > 0:
                wrist_3d = left_hand_landmarks[0]  # Wrist is landmark 0
                left_points_3d.append(wrist_3d)

            # Add right hand wrist (landmark 0) if available
            if len(right_hand_landmarks) > 0:
                wrist_3d = right_hand_landmarks[0]  # Wrist is landmark 0
                right_points_3d.append(wrist_3d)

        # Draw left hand path in 3D
        if len(left_points_3d) > 1:
            path_array = np.array(left_points_3d)

            # Draw main path line
            ax.plot(path_array[:, 0], path_array[:, 1], path_array[:, 2],
                    color=self.path_colors['left_hand'][:3],
                    alpha=self.path_colors['left_hand'][3],
                    linewidth=3, label='Left Hand Path (3D Calibrated)')

            # Add gradient effect by plotting segments with varying alpha
            num_segments = min(20, len(path_array) - 1)
            if num_segments > 0:
                segment_size = max(1, len(path_array) // num_segments)

                for i in range(0, len(path_array) - segment_size, segment_size):
                    end_idx = min(i + segment_size, len(path_array))
                    alpha = (i / len(path_array)) * 0.3 + 0.4  # Alpha from 0.4 to 0.7
                    ax.plot(path_array[i:end_idx, 0], path_array[i:end_idx, 1], path_array[i:end_idx, 2],
                            color=self.path_colors['left_hand'][:3], alpha=alpha, linewidth=2)

            # Mark start and end points
            if len(path_array) > 0:
                # Start point (green)
                ax.scatter([path_array[0, 0]], [path_array[0, 1]], [path_array[0, 2]],
                           c='green', s=100, marker='o', alpha=0.8, label='Left Start')
                # End point (red)
                ax.scatter([path_array[-1, 0]], [path_array[-1, 1]], [path_array[-1, 2]],
                           c='red', s=100, marker='s', alpha=0.8, label='Left Current')

        # Draw right hand path in 3D
        if len(right_points_3d) > 1:
            path_array = np.array(right_points_3d)

            # Draw main path line
            ax.plot(path_array[:, 0], path_array[:, 1], path_array[:, 2],
                    color=self.path_colors['right_hand'][:3],
                    alpha=self.path_colors['right_hand'][3],
                    linewidth=3, label='Right Hand Path (3D Calibrated)')

            # Add gradient effect
            num_segments = min(20, len(path_array) - 1)
            if num_segments > 0:
                segment_size = max(1, len(path_array) // num_segments)

                for i in range(0, len(path_array) - segment_size, segment_size):
                    end_idx = min(i + segment_size, len(path_array))
                    alpha = (i / len(path_array)) * 0.3 + 0.4  # Alpha from 0.4 to 0.7
                    ax.plot(path_array[i:end_idx, 0], path_array[i:end_idx, 1], path_array[i:end_idx, 2],
                            color=self.path_colors['right_hand'][:3], alpha=alpha, linewidth=2)

            # Mark start and end points
            if len(path_array) > 0:
                # Start point (green)
                ax.scatter([path_array[0, 0]], [path_array[0, 1]], [path_array[0, 2]],
                           c='green', s=100, marker='o', alpha=0.8, label='Right Start')
                # End point (blue)
                ax.scatter([path_array[-1, 0]], [path_array[-1, 1]], [path_array[-1, 2]],
                           c='blue', s=100, marker='s', alpha=0.8, label='Right Current')

    def get_path_statistics(self):
        """Get statistics about hand movements"""
        stats = {
            'left_hand': {
                'total_points': len(self.full_left_hand_path),
                'total_distance': 0.0,
                'avg_speed': 0.0,
                'max_speed': 0.0
            },
            'right_hand': {
                'total_points': len(self.full_right_hand_path),
                'total_distance': 0.0,
                'avg_speed': 0.0,
                'max_speed': 0.0
            }
        }

        # Calculate distances and speeds for left hand
        if len(self.full_left_hand_path) > 1:
            distances = []
            speeds = []
            for i in range(1, len(self.full_left_hand_path)):
                p1 = np.array(self.full_left_hand_path[i - 1]['point'])
                p2 = np.array(self.full_left_hand_path[i]['point'])
                distance = np.linalg.norm(p2 - p1)
                distances.append(distance)

                time_diff = self.full_left_hand_path[i]['timestamp'] - self.full_left_hand_path[i - 1]['timestamp']
                if time_diff > 0:
                    speed = distance / time_diff
                    speeds.append(speed)

            stats['left_hand']['total_distance'] = sum(distances)
            if speeds:
                stats['left_hand']['avg_speed'] = np.mean(speeds)
                stats['left_hand']['max_speed'] = np.max(speeds)

        # Calculate distances and speeds for right hand
        if len(self.full_right_hand_path) > 1:
            distances = []
            speeds = []
            for i in range(1, len(self.full_right_hand_path)):
                p1 = np.array(self.full_right_hand_path[i - 1]['point'])
                p2 = np.array(self.full_right_hand_path[i]['point'])
                distance = np.linalg.norm(p2 - p1)
                distances.append(distance)

                time_diff = self.full_right_hand_path[i]['timestamp'] - self.full_right_hand_path[i - 1]['timestamp']
                if time_diff > 0:
                    speed = distance / time_diff
                    speeds.append(speed)

            stats['right_hand']['total_distance'] = sum(distances)
            if speeds:
                stats['right_hand']['avg_speed'] = np.mean(speeds)
                stats['right_hand']['max_speed'] = np.max(speeds)

        return stats


def visualize_landmarks_3d(json_path: str, mode: str = 'static', frame_number: Optional[int] = None,
                           save_path: Optional[str] = None, track_hands: bool = False,
                           initial_view: str = 'z_axis'):
    """
    Visualize landmarks from video_landmarks.json in 3D space

    Args:
        json_path: Path to the video_landmarks.json file
        mode: 'static' for single frame, 'animated' for animation
        frame_number: Specific frame to show (for static mode), None for first frame
        save_path: Path to save animation (optional, for animated mode)
        track_hands: Whether to enable hand path tracking with consistency correction
        initial_view: Initial viewing angle ('z_axis', 'perspective', or tuple of (elev, azim))
    """

    try:
        visualizer = LandmarksVisualizer3D(json_path, track_hands=track_hands)

        # Show analysis if hand tracking is enabled
        if track_hands:
            visualizer.analyze_hand_paths()

        if mode == 'static':
            visualizer.visualize_static(frame_number)
        elif mode == 'animated':
            if save_path:
                visualizer.save_animation(save_path)
            else:
                visualizer.visualize_animated()
        else:
            raise ValueError("Mode must be 'static' or 'animated'")

    except Exception as e:
        print(f"Error in visualization: {e}")
        raise


def main():
    """Command line interface for 3D visualization"""
    parser = argparse.ArgumentParser(description='Visualize sign language landmarks in 3D')
    parser.add_argument('json_path', help='Path to video_landmarks.json file')
    parser.add_argument('--mode', choices=['static', 'animated'], default='static',
                        help='Visualization mode (default: static)')
    parser.add_argument('--frame', type=int, default=0,
                        help='Frame number to display (for static mode, default: 0)')
    parser.add_argument('--save', type=str,
                        help='Save animation to file (for animated mode)')
    parser.add_argument('--track-hands', action='store_true',
                        help='Enable hand path visualization with left/right consistency correction')

    args = parser.parse_args()

    print(f"Loading landmarks from: {args.json_path}")
    print(f"Mode: {args.mode}")
    print(f"Hand tracking: {'ON' if args.track_hands else 'OFF'}")

    if args.mode == 'static':
        print(f"Displaying frame: {args.frame}")

    visualize_landmarks_3d(args.json_path, args.mode, args.frame, args.save, args.track_hands)


if __name__ == "__main__":
    main()
